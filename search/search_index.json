{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vertex Pipelines Deployer","text":"Deploy Vertex Pipelines within minutes <p>         This tool is a wrapper around kfp and google-cloud-aiplatform that allows you to check, compile, upload, run and schedule Vertex Pipelines in a standardized manner.         </p> <p></p> <p>Info</p> <p>This project is looking for beta testers and contributors.</p> <p>You can contact code owners or submit a new issue if you want to help.</p>"},{"location":"#why-this-tool","title":"Why this tool?","text":"<p>Three use cases:</p> <ol> <li>CI: Check pipeline validity.</li> <li>Dev mode: Quickly iterate over your pipelines by compiling and running them in multiple environments (test, dev, staging, etc.) without duplicating code or searching for the right kfp/aiplatform snippet.</li> <li>CD: Deploy your pipelines to Vertex Pipelines in a standardized manner in your CD with Cloud Build or GitHub Actions.</li> </ol> <p>Two main commands:</p> <ul> <li><code>check</code>: Check your pipelines (imports, compile, check configs validity against pipeline definition).</li> <li><code>deploy</code>: Compile, upload to Artifact Registry, run, and schedule your pipelines.</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#from-pypi","title":"From PyPI","text":"<pre><code>pip install vertex-deployer\n</code></pre>"},{"location":"#from-git-repo","title":"From git repo","text":"<p>Stable version: <pre><code>pip install git+https://github.com/artefactory/vertex-pipelines-deployer.git@main\n</code></pre></p> <p>Develop version: <pre><code>pip install git+https://github.com/artefactory/vertex-pipelines-deployer.git@develop\n</code></pre></p> <p>If you want to test this package on examples from this repo: <pre><code>git clone git@github.com:artefactory/vertex-pipelines-deployer.git\npoetry install\npoetry shell  # if you want to activate the virtual environment\ncd example\n</code></pre></p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Please refer to the Basic Usage page for more information.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Apache 2.0.</p>"},{"location":"CLI_REFERENCE/","title":"<code>vertex-deployer</code>","text":"<p>Usage:</p> <pre><code>$ vertex-deployer [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>-log, --log-level [TRACE|DEBUG|INFO|SUCCESS|WARNING|ERROR|CRITICAL]</code>: Set the logging level.  [default: INFO]</li> <li><code>-v, --version</code>: Display the version number and exit.</li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>check</code>: Check that pipelines are valid.</li> <li><code>config</code>: Display the configuration from...</li> <li><code>create</code>: Create files structure for a new pipeline.</li> <li><code>deploy</code>: Compile, upload, run and schedule pipelines.</li> <li><code>init</code>: Initialize the deployer.</li> <li><code>list</code>: List all pipelines.</li> </ul>"},{"location":"CLI_REFERENCE/#vertex-deployer-check","title":"<code>vertex-deployer check</code>","text":"<p>Check that pipelines are valid.</p> <p>Checking that a pipeline is valid includes:</p> <ul> <li> <p>Checking that the pipeline can be imported. It must be a valid python module with a <code>{pipeline_name}</code> function decorated with <code>@kfp.dsl.pipeline</code>.</p> </li> <li> <p>Checking that the pipeline can be compiled using <code>kfp.compiler.Compiler</code>.</p> </li> <li> <p>Checking that config files in <code>{configs_root_path}/{pipeline_name}</code> are corresponding to the pipeline parameters definition, using Pydantic.</p> </li> </ul> <p>This command can be used to check pipelines in a Continuous Integration workflow.</p> <p>Usage:</p> <pre><code>$ vertex-deployer check [OPTIONS] [PIPELINE_NAMES]...\n</code></pre> <p>Arguments:</p> <ul> <li><code>[PIPELINE_NAMES]...</code>: The names of the pipeline to check.</li> </ul> <p>Options:</p> <ul> <li><code>-a, --all</code>: Whether to check all pipelines.</li> <li><code>-cfp, --config-filepath FILE</code>: Path to the json/py file with parameter values and input artifactsto check. If not specified, all config files in the pipeline dir will be checked.</li> <li><code>-re, --raise-error / -nre, --no-raise-error</code>: Whether to raise an error if the pipeline is not valid.  [default: no-raise-error]</li> <li><code>-wd, --warn-defaults / -nwd, --no-warn-defaults</code>: Whether to warn when a default value is used.and not overwritten in config file.  [default: warn-defaults]</li> <li><code>-rfd, --raise-for-defaults / -nrfd, --no-raise-for-defaults</code>: Whether to raise an validation error when a default value is used.and not overwritten in config file.  [default: no-raise-for-defaults]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"CLI_REFERENCE/#vertex-deployer-config","title":"<code>vertex-deployer config</code>","text":"<p>Display the configuration from pyproject.toml.</p> <p>Usage:</p> <pre><code>$ vertex-deployer config [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-a, --all</code>: Whether to display all configuration values.</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"CLI_REFERENCE/#vertex-deployer-create","title":"<code>vertex-deployer create</code>","text":"<p>Create files structure for a new pipeline.</p> <p>Usage:</p> <pre><code>$ vertex-deployer create [OPTIONS] PIPELINE_NAMES...\n</code></pre> <p>Arguments:</p> <ul> <li><code>PIPELINE_NAMES...</code>: The names of the pipeline to create.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-ct, --config-type [json|py|toml|yaml]</code>: The type of the config to create.  [default: yaml]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"CLI_REFERENCE/#vertex-deployer-deploy","title":"<code>vertex-deployer deploy</code>","text":"<p>Compile, upload, run and schedule pipelines.</p> <p>Usage:</p> <pre><code>$ vertex-deployer deploy [OPTIONS] PIPELINE_NAMES...\n</code></pre> <p>Arguments:</p> <ul> <li><code>PIPELINE_NAMES...</code>: The names of the pipeline to run.  [required]</li> </ul> <p>Options:</p> <ul> <li><code>--env-file FILE</code>: The environment file to use.</li> <li><code>-c, --compile / -nc, --no-compile</code>: Whether to compile the pipeline.  [default: compile]</li> <li><code>-u, --upload / -nu, --no-upload</code>: Whether to upload the pipeline to Google Artifact Registry.  [default: no-upload]</li> <li><code>-r, --run / -nr, --no-run</code>: Whether to run the pipeline.  [default: no-run]</li> <li><code>-s, --schedule / -ns, --no-schedule</code>: Whether to create a schedule for the pipeline.  [default: no-schedule]</li> <li><code>--cron TEXT</code>: Cron expression for scheduling the pipeline. To pass it to the CLI, use underscore e.g. '0_10___*'.</li> <li><code>-dls, --delete-last-schedule</code>: Whether to delete the previous schedule before creating a new one.</li> <li><code>--scheduler-timezone TEXT</code>: Timezone for scheduling the pipeline. Must be a valid string from IANA time zone database  [default: Europe/Paris]</li> <li><code>--tags TEXT</code>: The tags to use when uploading the pipeline.</li> <li><code>-cfp, --config-filepath FILE</code>: Path to the json/py file with parameter values and input artifacts to use when running the pipeline.</li> <li><code>-cn, --config-name TEXT</code>: Name of the json/py file with parameter values and input artifacts to use when running the pipeline. It must be in the pipeline config dir. e.g. <code>config_dev.json</code> for <code>./vertex/configs/{pipeline-name}/config_dev.json</code>.</li> <li><code>-ec, --enable-caching / -nec, --no-cache</code>: Whether to turn on caching for the run.If this is not set, defaults to the compile time settings, which are True for alltasks by default, while users may specify different caching options for individualtasks. If this is set, the setting applies to all tasks in the pipeline.Overrides the compile time settings. Defaults to None.</li> <li><code>-en, --experiment-name TEXT</code>: The name of the experiment to run the pipeline in.Defaults to '{pipeline_name}-experiment'.</li> <li><code>-rn, --run-name TEXT</code>: The pipeline's run name. Displayed in the UI.Defaults to '{pipeline_name}-{tags}-%Y%m%d%H%M%S'.</li> <li><code>-y, --skip-validation / -n, --no-skip</code>: Whether to continue without user validation of the settings.  [default: skip-validation]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"CLI_REFERENCE/#vertex-deployer-init","title":"<code>vertex-deployer init</code>","text":"<p>Initialize the deployer.</p> <p>Usage:</p> <pre><code>$ vertex-deployer init [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-d, --default</code>: Instantly creates the full vertex structure and files without configuration prompts</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"CLI_REFERENCE/#vertex-deployer-list","title":"<code>vertex-deployer list</code>","text":"<p>List all pipelines.</p> <p>Usage:</p> <pre><code>$ vertex-deployer list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-wc, --with-configs / -nc , --no-configs</code>: Whether to list config files.  [default: no-configs]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"changelog/","title":"CHANGELOG","text":""},{"location":"changelog/#055-2025-01-25","title":"0.5.5 (2025-01-25)","text":""},{"location":"changelog/#build-system","title":"Build system","text":"<ul> <li>build(deps): bump actions/cache from 4.0.0 to 4.1.1 (#215)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>cf0a5d9</code>)</p> <ul> <li>build(deps): bump python-semantic-release/python-semantic-release from 9.6.0 to 9.11.0 (#216)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: julesbertrand julesbertrand13@gmail.com (<code>7c0635c</code>)</p> <ul> <li> <p>build(deps): actions/pr-agent set version to 0.24 (#218) (<code>1058103</code>)</p> </li> <li> <p>build(deps): update pyinstrument requirement from ^4.5 to &gt;=4.5,&lt;6.0 (#214)</p> </li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>d618659</code>)</p> <ul> <li>build(deps): update tomlkit requirement from ^0.12 to &gt;=0.12,&lt;0.14 (#200)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>575c4a9</code>)</p>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li> <p>docs: add api ref using mkdocstrings (#231) (<code>efef2de</code>)</p> </li> <li> <p>docs: fix typo in deployer toml configuration (<code>489ecac</code>)</p> </li> </ul>"},{"location":"changelog/#enhancements","title":"Enhancements","text":"<ul> <li>enh: use underscore instead of dash in cron parsing (#234) (<code>7221d6a</code>)</li> </ul>"},{"location":"changelog/#054-2024-10-11","title":"0.5.4 (2024-10-11)","text":""},{"location":"changelog/#build-system_1","title":"Build system","text":"<ul> <li> <p>build(deps): add compatibility with python up to 3.12 (#192) (<code>76cd116</code>)</p> </li> <li> <p>build(deps-dev): update ruff requirement from ^0.3 to &gt;=0.3,&lt;0.6 (#196) (<code>03549f7</code>)</p> </li> <li> <p>build(deps-dev): update pytest-cov requirement from ^4.1 to ^5.0 (#168) (<code>d0305ec</code>)</p> </li> </ul>"},{"location":"changelog/#continuous-integration","title":"Continuous integration","text":"<ul> <li>ci: update semantic release branches rules (#207) (<code>48ab5f7</code>)</li> </ul>"},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>docs: update to skaff style (#210) (<code>ab5fb2f</code>)</li> </ul>"},{"location":"changelog/#enhancements_1","title":"Enhancements","text":"<ul> <li>enh: ask for config file type in init (#206) (<code>79a66e2</code>)</li> </ul>"},{"location":"changelog/#fixes","title":"Fixes","text":"<ul> <li>fix: yaml and init templates (#205) (<code>0885793</code>)</li> </ul>"},{"location":"changelog/#053-2024-07-04","title":"0.5.3 (2024-07-04)","text":""},{"location":"changelog/#fixes_1","title":"Fixes","text":"<ul> <li>fix: pydantic update 2.8.0 (#197) (<code>0127301</code>)</li> </ul>"},{"location":"changelog/#052-2024-06-14","title":"0.5.2 (2024-06-14)","text":""},{"location":"changelog/#enhancements_2","title":"Enhancements","text":"<ul> <li>enh: add yaml support (#193) (<code>d00f518</code>)</li> </ul>"},{"location":"changelog/#051-2024-06-06","title":"0.5.1 (2024-06-06)","text":""},{"location":"changelog/#chores","title":"Chores","text":"<ul> <li>chore: ak repo explo (#186) (<code>20e6665</code>)</li> </ul>"},{"location":"changelog/#continuous-integration_1","title":"Continuous integration","text":"<ul> <li>ci: set python semantic release version (<code>9be89db</code>)</li> </ul>"},{"location":"changelog/#enhancements_3","title":"Enhancements","text":"<ul> <li>enh: rename pipeline UI (#187) (<code>7b1ae60</code>)</li> </ul>"},{"location":"changelog/#unknown","title":"Unknown","text":"<ul> <li>improvement: add tests init (#182)</li> </ul> <p>Co-authored-by: Elie Trigano elie.trigano@artefact.com (<code>89f9701</code>)</p>"},{"location":"changelog/#050-2024-04-16","title":"0.5.0 (2024-04-16)","text":""},{"location":"changelog/#documentation_2","title":"Documentation","text":"<ul> <li>docs: improve readme badges (#180) (<code>8e1faf8</code>)</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>feat: improve init (#173)</li> </ul> <p>Co-authored-by: Elie Trigano elie.trigano@artefact.com (<code>f937dfe</code>)</p>"},{"location":"changelog/#044-2024-04-12","title":"0.4.4 (2024-04-12)","text":""},{"location":"changelog/#build-system_2","title":"Build system","text":"<ul> <li>build(deps): bump vertex-deployer from 0.4.0 to 0.4.3 (#166)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: julesbertrand julesbertrand13@gmail.com (<code>2cd0fab</code>)</p> <ul> <li>build(deps): update typer requirement from ^0.9 to ^0.12 (#171)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: julesbertrand julesbertrand13@gmail.com (<code>71c9da5</code>)</p> <ul> <li>build(deps-dev): update ruff requirement from ^0.1 to ^0.3 (#165)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: julesbertrand julesbertrand13@gmail.com (<code>2a5b637</code>)</p>"},{"location":"changelog/#chores_1","title":"Chores","text":"<ul> <li> <p>chore: better exception message when validating deployer settings (#163) (<code>5c08106</code>)</p> </li> <li> <p>chore: create .skaff/skaff.yaml (#172) (<code>9876d39</code>)</p> </li> </ul>"},{"location":"changelog/#continuous-integration_2","title":"Continuous integration","text":"<ul> <li> <p>ci: check cli documentation is up to date (#178) (<code>4c6436c</code>)</p> </li> <li> <p>ci: upload to pypi (#167) (<code>7f02b42</code>)</p> </li> </ul>"},{"location":"changelog/#documentation_3","title":"Documentation","text":"<ul> <li>docs: update docs for public release (#174) (<code>13219d0</code>)</li> </ul>"},{"location":"changelog/#fixes_2","title":"Fixes","text":"<ul> <li>fix: enable caching default behavior (#177) (<code>34663c1</code>)</li> </ul>"},{"location":"changelog/#043-2024-03-01","title":"0.4.3 (2024-03-01)","text":""},{"location":"changelog/#continuous-integration_3","title":"Continuous integration","text":"<ul> <li>ci: fix deploy docs workflow trigger (#156) (<code>32351db</code>)</li> </ul>"},{"location":"changelog/#fixes_3","title":"Fixes","text":"<ul> <li> <p>fix: instanciate list only if not ixisting when paring errors (#161) (<code>16028a2</code>)</p> </li> <li> <p>fix: inspect value empty for defaults in check cmd (#159) (<code>5536f67</code>)</p> </li> </ul>"},{"location":"changelog/#042-2024-02-26","title":"0.4.2 (2024-02-26)","text":""},{"location":"changelog/#continuous-integration_4","title":"Continuous integration","text":"<ul> <li> <p>ci: update changelog template (#154) (<code>0f4e8c0</code>)</p> </li> <li> <p>ci: deploy docs when push on main (#151) (<code>5a77249</code>)</p> </li> </ul>"},{"location":"changelog/#documentation_4","title":"Documentation","text":"<ul> <li>docs: update cli reference (#150) (<code>8925729</code>)</li> </ul>"},{"location":"changelog/#enhancements_4","title":"Enhancements","text":"<ul> <li> <p>enh: deploy multiple pipelines at once (#149) (<code>64aa9cd</code>)</p> </li> <li> <p>enh: handle default values (#148) (<code>032ef87</code>)</p> </li> </ul>"},{"location":"changelog/#fixes_4","title":"Fixes","text":"<ul> <li>fix: init command failing (#147) (<code>df3fb3a</code>)</li> </ul>"},{"location":"changelog/#041-2024-02-14","title":"0.4.1 (2024-02-14)","text":""},{"location":"changelog/#build-system_3","title":"Build system","text":"<ul> <li> <p>build(deps-dev): update black requirement from ^23.7 to ^24.1 (#138) (<code>ae8ee8c</code>)</p> </li> <li> <p>build(deps): bump actions/cache from 3.3.2 to 4.0.0 (#135) (<code>3584598</code>)</p> </li> <li> <p>build(deps): bump vertex-deployer from 0.3.2 to 0.4.0 (#131) (<code>bd2c1b5</code>)</p> </li> <li> <p>build(deps-dev): update pytest requirement from ^7.4 to ^8.0 (#137) (<code>9d5a148</code>)</p> </li> </ul>"},{"location":"changelog/#continuous-integration_5","title":"Continuous integration","text":"<ul> <li>ci: use ruff for formatting (#144) (<code>ba554c3</code>)</li> </ul>"},{"location":"changelog/#fixes_5","title":"Fixes","text":"<ul> <li> <p>fix: print vertex settings and ask for validation (#141) (<code>668aaad</code>)</p> </li> <li> <p>fix: traceback enhancement (#139) (<code>f71f443</code>)</p> </li> </ul>"},{"location":"changelog/#040-2024-01-05","title":"0.4.0 (2024-01-05)","text":""},{"location":"changelog/#build-system_4","title":"Build system","text":"<ul> <li>build(release): fix release to gcp (#126) (<code>9b4e619</code>)</li> </ul>"},{"location":"changelog/#chores_2","title":"Chores","text":"<ul> <li>chore: switch to apache 2.0 licence (#128) (<code>ea3a67d</code>)</li> </ul>"},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>feat: add init command to configure deployer (#127) (<code>5ac7022</code>)</li> </ul>"},{"location":"changelog/#performance-improvements","title":"Performance improvements","text":"<ul> <li>perf: improve cli with context and callbacks (#129) (<code>d99c007</code>)</li> </ul>"},{"location":"changelog/#033-2024-01-03","title":"0.3.3 (2024-01-03)","text":""},{"location":"changelog/#build-system_5","title":"Build system","text":"<ul> <li> <p>build(deps): bump actions/setup-python from 4 to 5 (#118) (<code>3d48df3</code>)</p> </li> <li> <p>build(deps): bump google-github-actions/auth from 1 to 2 (#114)</p> </li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>d98a54e</code>)</p> <ul> <li>build(deps): bump vertex-deployer from 0.3.1 to 0.3.2 (#107) (<code>9af430b</code>)</li> </ul>"},{"location":"changelog/#chores_3","title":"Chores","text":"<ul> <li>chore: update artifact registry url (#106) (<code>226f200</code>)</li> </ul>"},{"location":"changelog/#continuous-integration_6","title":"Continuous integration","text":"<ul> <li> <p>ci: add pep-8 compliance and bandit ruff checks (#110) (<code>c87685b</code>)</p> </li> <li> <p>ci: upload to gar only if release (#109) (<code>38a8c79</code>)</p> </li> <li> <p>ci: update workflows names (#104) (<code>66ffd4d</code>)</p> </li> <li> <p>ci: add branch protection rules to main (#102) (<code>97f8a2e</code>)</p> </li> </ul>"},{"location":"changelog/#documentation_5","title":"Documentation","text":"<ul> <li> <p>docs: clean documentation (#124) (<code>cdf1db0</code>)</p> </li> <li> <p>docs: update catalog info (#111) (<code>53b9004</code>)</p> </li> <li> <p>docs: fix hyperlinks and toc (#105) (<code>b229c28</code>)</p> </li> <li> <p>docs: update badges and doc index (#101) (<code>a17ec3c</code>)</p> </li> <li> <p>docs: update doc build trigger (#100) (<code>9560b70</code>)</p> </li> </ul>"},{"location":"changelog/#fixes_6","title":"Fixes","text":"<ul> <li> <p>fix: toml config files cannot have sections (#122) (<code>013e2ba</code>)</p> </li> <li> <p>fix: multiple issues when using vertex deployer create (#123) (<code>b98b14e</code>)</p> </li> </ul>"},{"location":"changelog/#testing","title":"Testing","text":"<ul> <li>test: add integration test to ensure config and cli params are iso (#121) (<code>440b51e</code>)</li> </ul>"},{"location":"changelog/#032-2023-11-08","title":"0.3.2 (2023-11-08)","text":""},{"location":"changelog/#build-system_6","title":"Build system","text":"<ul> <li>build(deps-dev): update ruff requirement from ^0.0 to ^0.1 (#95)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>cff19af</code>)</p> <ul> <li>build(deps): bump actions/cache from 3.2.4 to 3.3.2 (#94)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>5d2dbf9</code>)</p> <ul> <li>build(deps): bump actions/setup-python from 2 to 4 (#92)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>9d25ad6</code>)</p> <ul> <li>build(deps): bump actions/checkout from 2 to 4 (#93)</li> </ul> <p>Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com (<code>c97ec85</code>)</p> <ul> <li>build: update dependencies specifiers to be more flexible (#91) (<code>fddf1a9</code>)</li> </ul>"},{"location":"changelog/#continuous-integration_7","title":"Continuous integration","text":"<ul> <li>ci: build and deploy docs when a release is published (#90) (<code>01ce99a</code>)</li> </ul>"},{"location":"changelog/#documentation_6","title":"Documentation","text":"<ul> <li> <p>docs: add full example to repo (#72) (<code>42ccfc9</code>)</p> </li> <li> <p>docs: update installation instructions (#98) (<code>6e51b89</code>)</p> </li> </ul>"},{"location":"changelog/#fixes_7","title":"Fixes","text":"<ul> <li>fix: warn when cannot associate run to experiment (#99) (<code>7ba89f3</code>)</li> </ul>"},{"location":"changelog/#performance-improvements_1","title":"Performance improvements","text":"<ul> <li>perf: rationalize kfp imports (#97) (<code>19da429</code>)</li> </ul>"},{"location":"changelog/#031-2023-11-06","title":"0.3.1 (2023-11-06)","text":""},{"location":"changelog/#fixes_8","title":"Fixes","text":"<ul> <li> <p>fix: build doc after release (#89) (<code>e9b6b36</code>)</p> </li> <li> <p>fix: run option fails when not uploading to gar (#87) (<code>6fb7c14</code>)</p> </li> </ul>"},{"location":"changelog/#030-2023-11-06","title":"0.3.0 (2023-11-06)","text":""},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>feat: configure with pyproject (#82) (<code>56d69f4</code>)</li> </ul>"},{"location":"changelog/#022-2023-11-06","title":"0.2.2 (2023-11-06)","text":""},{"location":"changelog/#continuous-integration_8","title":"Continuous integration","text":"<ul> <li>ci: update changelog parameters (#81) (<code>5cd8321</code>)</li> </ul>"},{"location":"changelog/#documentation_7","title":"Documentation","text":"<ul> <li>docs: add mkdocs documentation (#79) (<code>dd66594</code>)</li> </ul>"},{"location":"changelog/#fixes_9","title":"Fixes","text":"<ul> <li> <p>fix: release to artifact registry (#85) (<code>395b256</code>)</p> </li> <li> <p>fix: rm protected namespaces from pipeline model (#78) (<code>7ea8975</code>)</p> </li> <li> <p>fix: base unsupported config file error msg on config types enum (#77) (<code>b3f3b46</code>)</p> </li> </ul>"},{"location":"changelog/#021-2023-10-13","title":"0.2.1 (2023-10-13)","text":""},{"location":"changelog/#documentation_8","title":"Documentation","text":"<ul> <li>docs: fix typos in readme (#74) (<code>9cc7ad3</code>)</li> </ul>"},{"location":"changelog/#fixes_10","title":"Fixes","text":"<ul> <li> <p>fix: simplify check pipelines cmd (#73) (<code>c63a7fb</code>)</p> </li> <li> <p>fix: add pipeline root path check in create command (#75) (<code>65977a0</code>)</p> </li> </ul>"},{"location":"changelog/#020-2023-10-06","title":"0.2.0 (2023-10-06)","text":""},{"location":"changelog/#documentation_9","title":"Documentation","text":"<ul> <li>docs: update documentation (#68) (<code>e942add</code>)</li> </ul>"},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>feat: add support for toml config files (#70) (<code>b997e69</code>)</li> </ul>"},{"location":"changelog/#fixes_11","title":"Fixes","text":"<ul> <li> <p>fix: pipelines objects can be named as <code>{pipeline_name}</code> instead of <code>pipeline</code> (#69) (<code>f79d081</code>)</p> </li> <li> <p>fix: bad config error at pipeline level in checks (#67) (<code>d8dab84</code>)</p> </li> </ul>"},{"location":"changelog/#011-2023-10-05","title":"0.1.1 (2023-10-05)","text":""},{"location":"changelog/#documentation_10","title":"Documentation","text":"<ul> <li>docs: update installation guidelines (#61) (<code>e284168</code>)</li> </ul>"},{"location":"changelog/#fixes_12","title":"Fixes","text":"<ul> <li> <p>fix: make imports in cli commands to reduce overhead (#63) (<code>9c973f0</code>)</p> </li> <li> <p>fix: checks temp directory removal (#62) (<code>f005f44</code>)</p> </li> </ul>"},{"location":"changelog/#010-2023-10-05","title":"0.1.0 (2023-10-05)","text":""},{"location":"changelog/#chores_4","title":"Chores","text":"<ul> <li>chore: update release version tag format and commit message (#50) (<code>b635287</code>)</li> </ul>"},{"location":"changelog/#continuous-integration_9","title":"Continuous integration","text":"<ul> <li> <p>ci: upload release to gcs (#56)</p> </li> <li> <p>ci: update version_variables and changelog patterns for release</p> </li> <li> <p>ci: upload release to gcs bucket</p> </li> <li> <p>ci: update ci actions version (<code>e7119db</code>)</p> </li> <li> <p>ci: update ci trigger policy (#45) (<code>f1171d2</code>)</p> </li> </ul>"},{"location":"changelog/#enhancements_5","title":"Enhancements","text":"<ul> <li>enh: use pydantic settings to get deployment variables from env file instead of os.environ (#24) (<code>879c14a</code>)</li> </ul>"},{"location":"changelog/#features_4","title":"Features","text":"<ul> <li> <p>feat: add rich display in console (#54)</p> </li> <li> <p>feat: add console status for deploy command and console output for check</p> </li> <li> <p>enh: remove empty columns from rich table</p> </li> <li> <p>fix: rm time.sleep from code</p> </li> <li> <p>enh: use dataclass as row for pipeline checks and group errors by config path</p> </li> <li> <p>fix: make rich mandatory as dependency</p> </li> <li> <p>fix: typing error in python 3.8</p> </li> <li> <p>enh: rename config_path as config_file (<code>6753402</code>)</p> </li> </ul>"},{"location":"changelog/#fixes_13","title":"Fixes","text":"<ul> <li> <p>fix: scheduling tag retrieval (#59) (<code>11347ba</code>)</p> </li> <li> <p>fix: misc code improvements (#58)</p> </li> <li> <p>enh: add version callback for app</p> </li> <li> <p>test: add integration test for root command in CI</p> </li> <li> <p>doc: update README with installation from gcs guidelines</p> </li> <li> <p>doc: fix typos in readme</p> </li> <li> <p>enh: add possibility to use either --config-filepath or --config-name</p> </li> <li> <p>fix: log the right experiment name</p> </li> <li> <p>fix: check that cron arg is not empty string</p> </li> <li> <p>doc: add doc about cron job format in cli</p> </li> <li> <p>feat: add rich display for pipeline list command</p> </li> <li> <p>chore: rename version_callback to display_version_and_exit</p> </li> <li> <p>fix: typer bad parameter raised immediatelty after cli call in deploy command (<code>0226088</code>)</p> </li> <li> <p>fix: misc typing and logging typos (#52)</p> </li> <li> <p>fix: logging disable in checks</p> </li> <li> <p>fix: VertexPipelineDeployer type hints and paths construction (<code>c80aeb1</code>)</p> </li> <li> <p>fix: rm unused files (#5) (<code>e220dc8</code>)</p> </li> <li> <p>fix: readme typos (#4) (<code>3ebcf4a</code>)</p> </li> </ul>"},{"location":"changelog/#unknown_1","title":"Unknown","text":"<ul> <li>0.0.1</li> </ul> <p>Automatically generated by python-semantic-release (<code>0809df7</code>)</p> <ul> <li> <p>Release v0.1.0 (#48) (<code>a3c18df</code>)</p> </li> <li> <p>Ci: Update Continuous Deployment (CD) Trigger Policy and Documentation (#47)</p> </li> <li> <p>ci: update cd trigger policy</p> </li> <li> <p>ci: update cd doc</p> </li> <li> <p>ci: test reusable ci</p> </li> <li> <p>ci: fix reusable ci ref</p> </li> <li> <p>ci: fix reusable ci</p> </li> <li> <p>ci: add need for CI to be completed</p> </li> <li> <p>ci: fix cd on main to be triggered only when pushing to main</p> </li> <li> <p>ci: update doc for cd (<code>a08b581</code>)</p> </li> <li> <p>Chore: prepare for release (#38)</p> </li> <li> <p>chore: add release drafter</p> </li> <li> <p>chore: add release drafter</p> </li> <li> <p>chore: add init with version</p> </li> <li> <p>ci: update release action</p> </li> <li> <p>ci: update release action linting</p> </li> <li> <p>ci: add semantic release configuration</p> </li> <li> <p>doc: update CONTRIBUTING.md for release management (<code>f59b795</code>)</p> </li> <li> <p>Feat: make it python 38 39 compatible (#41)</p> </li> <li> <p>feat: typing back to 3.8</p> </li> <li> <p>doc: update readme with new python versions</p> </li> <li> <p>fix: update ci with new python versions (<code>4e50c99</code>)</p> </li> <li> <p>Feat: add cli checks to ci (#40)</p> </li> <li> <p>feat: add cli integration tets to ci</p> </li> <li> <p>fix: use poetry to run commands in ci</p> </li> <li> <p>fix: fix paths before launching cli commands (<code>9500a03</code>)</p> </li> <li> <p>Feat/add create and list commands (#39)</p> </li> <li> <p>feat: add comand list to list pipelines</p> </li> <li> <p>feat: add command create to cli and folder structure reorg</p> </li> <li> <p>enh: renamed pipelines_deployer.py -&gt; pipeline_deployer.py</p> </li> <li> <p>test: update tests</p> </li> <li> <p>doc: update readme</p> </li> <li> <p>enh: factorize get config paths (<code>9b973bf</code>)</p> </li> <li> <p>Test: add unit tests (#31)</p> </li> <li> <p>test: add tests for make_enum_from_python_package</p> </li> <li> <p>test: make them work</p> </li> <li> <p>test: add pytest cov</p> </li> <li> <p>fix: make file command name to run tests</p> </li> <li> <p>tests: add tests create_model_from_pipeline (<code>d01d60c</code>)</p> </li> <li> <p>Feat: pass artifacts as inputs (#28)</p> </li> <li> <p>feat: add argument input_artifacts_filepath to cli</p> </li> <li> <p>feat: add possibility to have python or json config files</p> </li> <li> <p>fix: update check command to support python files as config</p> </li> <li> <p>feat: allow to specify config path to check only one config file</p> </li> <li> <p>fix: change artifact type in pipeline dynamic model to allow valiation</p> </li> <li> <p>test: add tests to convert_artifact_type_to_str</p> </li> <li> <p>doc: update readme</p> </li> <li> <p>fix: change config file path option name</p> </li> <li> <p>enh: add and remove temp dir when checking pipelines (<code>4d163bd</code>)</p> </li> <li> <p>Fix/deploy command (#36)</p> </li> <li> <p>fix: iam rights for service account</p> </li> <li> <p>fix: multiple formatting issues when uploading pipeline template</p> </li> <li> <p>fix: typo in readme instruction for gcs bucket iam binding (<code>ead427f</code>)</p> </li> <li> <p>Feat/misc code improvements (#32)</p> </li> <li> <p>enh: use urljoin to make urls</p> </li> <li> <p>enh: add TagNotFoundError</p> </li> <li> <p>fix: vertex settings loading and errors</p> </li> <li> <p>enh: use decortor to check garhost in deployer</p> </li> <li> <p>enh: check experiment anme and check gar host</p> </li> <li> <p>feat: add missing gar host error</p> </li> <li> <p>feat: add message in no configs were checked for pipeline</p> </li> <li> <p>fix: path for pipeline should be relative not absolute</p> </li> <li> <p>fix: temp fix for vertex artifacts validation; arbitrary types allowed</p> </li> <li> <p>fix: upload does not work if lpp is not . (<code>94c8061</code>)</p> </li> <li> <p>Feat: add command to check pipelines (#19)</p> </li> <li> <p>feat: add comment to check pipelines (import, compile, config files)</p> </li> <li> <p>enh: creation of pipeline model only once</p> </li> <li> <p>feat: use pydantic to validate configs and get all validation errors in one exception</p> </li> <li> <p>feat: add error if no pipelines found in check and log of pipelines / config checked</p> </li> <li> <p>feat: add specific validator for import pipeline computed field (works as a property)</p> </li> <li> <p>doc: update docstring for  command</p> </li> <li> <p>doc: update readme and add --all flag</p> </li> <li> <p>doc: update README table of contents links</p> </li> <li> <p>feat: add context manager to disable loguru logger temporarily (<code>9f41c8e</code>)</p> </li> <li> <p>Feat: add pr_agent (#29)</p> </li> <li> <p>feat: add pr_agent</p> </li> <li> <p>feat: update pr agent action name (<code>92e1acb</code>)</p> </li> <li> <p>Fix: multiple issues raised in alpha testing (#27)</p> </li> <li> <p>fix: typos in code to make upload and run work</p> </li> <li> <p>doc: update readme</p> </li> <li> <p>doc: fix ruff and license badge</p> </li> <li> <p>doc: add why this tool in readme</p> </li> <li> <p>doc: add table of content</p> </li> <li> <p>enh: use --parameter-values-filepath instead of --config-name for clarity for user</p> </li> <li> <p>enh: put the vertex repository in example/</p> </li> <li> <p>doc: fix typo</p> </li> <li> <p>doc: update repo structure</p> </li> <li> <p>doc: update CONTRIBUTE.md (<code>05deb15</code>)</p> </li> <li> <p>Feat/switch logging to loguru (#20)</p> </li> <li> <p>enh: use loguru instead of python logging</p> </li> <li> <p>feat: add typer callback to set logging level (<code>6c65c09</code>)</p> </li> <li> <p>Fix/inconsistencies in pipeline names (#18)</p> </li> <li> <p>fix: use pipelines names with underscore instead of hyphen</p> </li> <li> <p>fix: rename module different from package</p> </li> <li> <p>doc: update readme accordingly (<code>7194c70</code>)</p> </li> <li> <p>Feat: switch cli to typer (#8)</p> </li> <li> <p>feat: switch cli to typer</p> </li> <li> <p>fix: add options short names + use enum value (<code>267d169</code>)</p> </li> <li> <p>Feat: add constants file (#7)</p> </li> <li> <p>feat: add constants file</p> </li> <li> <p>fix: package name in pyproject.toml</p> </li> <li> <p>fix: pr template contributing link (<code>54f59f7</code>)</p> </li> <li> <p>Chore: add issue and pr templates (#6)</p> </li> <li> <p>chore: add pr template</p> </li> <li> <p>chore: add issue templates</p> </li> <li> <p>chore: add CONTRIBUTING.md (<code>b736c3a</code>)</p> </li> <li> <p>Feat: vertex deployer (#3)</p> </li> <li> <p>feat/add vertex deployer and cli</p> </li> <li> <p>feat: add entrypoint for deployer</p> </li> <li> <p>fix: paths to pipeline folder and root path</p> </li> <li> <p>feat: add vertex foledr with dummy pipelines and example.env</p> </li> <li> <p>doc: update doc with how-to section (<code>f00c231</code>)</p> </li> <li> <p>Chore/update readme and add gitignore (#2)</p> </li> <li> <p>doc: update readme</p> </li> <li> <p>chore: add .gitignore (<code>3070873</code>)</p> </li> <li> <p>Chore: setup repo (#1)</p> </li> <li> <p>chore: setup repo</p> </li> <li> <p>fix: deployer is not a package error</p> </li> <li> <p>fix: rm pytest from prepush hooks</p> </li> <li> <p>chore: add to do list on the readme</p> </li> <li> <p>fix: add dummy test for the ci to pass (<code>f154389</code>)</p> </li> <li> <p>Initial commit (<code>cab9963</code>)</p> </li> </ul>"},{"location":"contributing/","title":"\ud83e\uddd1\u200d\ud83d\udcbb Contributing to Vertex Pipelines Deployer","text":""},{"location":"contributing/#how-to-contribute","title":"How to contribute","text":""},{"location":"contributing/#issues-pull-requests-and-code-reviews","title":"Issues, Pull Requests and Code Reviews.","text":"<p>Issues and Pull requests templates are mandatory.</p> <p>At least one code review is needed to merge. Please merge your feature branches on <code>develop</code>.</p> <p>We try to rebase as much as possible and use squash and merge to keep a linear and condensed git history.</p>"},{"location":"contributing/#getting-started","title":"Getting started","text":"<p>This project uses Poetry for dependency management. Poetry's doc is really good, so you should check it out if you have any questions.</p> <p>To install poetry:</p> <pre><code>make download-poetry\n</code></pre> <p>You can start by creating a virtual environment (conda or other) or use poetry venv(please check the Makefile first if so, as poetry venv is deactivated there). Then, to install the project dependencies, run the following command:</p> <pre><code>make install\n</code></pre> <p>To develop, you will need dev requirements too. Run: <pre><code>make install-dev-requirements\n</code></pre></p> <p>About poetry.lock</p> <p><code>poetry.lock</code> is not committed deliberately, as recommended by Poetry's doc. You can read more about it here.</p>"},{"location":"contributing/#codestyle","title":"Codestyle","text":"<p>This projects uses Black, isort, ruff for codestyle. You can run the following command to format your code. It uses Pre-commit hooks to run the formatters and linters.</p> <pre><code>make format-code\n</code></pre>"},{"location":"contributing/#docstring-convention","title":"Docstring convention","text":"<p>This project uses Google docstring convention.</p> <p>A full example is available in here.</p>"},{"location":"contributing/#how-to-release","title":"How to release","text":"<p>This project uses Python Semantic Versioning and Poetry to create releases and tags.</p> <p>The release process is automated through GitHub Actions. Here is the process:</p> <ul> <li>Create a Pull Request from <code>develop</code> to <code>main</code>.</li> <li>Merge the Pull Request. This must create a merge commit.</li> <li>The merge will trigger the Release GitHub Action defined in this workflow.</li> </ul> <p>The Release GitHub Action does the following:</p> <ul> <li>Checks out the code.</li> <li>Runs the CI GitHub Action, which runs the tests and linters.</li> <li>Runs Python Semantic Release, which takes care of version update, tag creation, and release creation.</li> </ul> <p>The action is triggered by any push to main.</p> <p>Tip</p> <p>The release action will be triggered by any push to <code>main</code> only if the 'CI' job in the 'release.yaml' workflow succeeds. Python Semantic Release will take care of version number update, tag creation and release creation.</p> <p>When it's done, rebase develop to keep it up to date with main.</p> <p>And you're done ! \ud83c\udf89</p>"},{"location":"example/","title":"Example","text":""},{"location":"example/#dummy-pipeline","title":"\ud83d\udea7 Dummy Pipeline","text":""},{"location":"example/#dev-compile-and-run-to-fasten-your-dev-cycle","title":"\ud83d\udea7 Dev: Compile and run to fasten your dev cycle","text":""},{"location":"example/#ci-check-your-pipelines-and-config-integrity","title":"\ud83d\udea7 CI: Check your pipelines and config integrity","text":""},{"location":"example/#cd-deploy-your-pipelines-in-a-standardized-manner","title":"\ud83d\udea7 CD: Deploy your pipelines in a standardized manner","text":""},{"location":"example/#github-action","title":"\ud83d\udea7 Github Action","text":""},{"location":"example/#cloudbuild-trigger","title":"\ud83d\udea7 CloudBuild trigger","text":""},{"location":"folder_structure/","title":"Folder Structure","text":"TL;DR <p>You need to respect the following file structure from Vertex Pipeline Starter Kit: <pre><code>vertex\n\u251c\u2500 configs/\n\u2502  \u2514\u2500 {pipeline_name}\n\u2502     \u2514\u2500 {config_name}.json\n\u2514\u2500 pipelines/\n    \u2514\u2500 {pipeline_name}.py\n</code></pre></p> <p>A pipeline file looks like this: <pre><code>```python title=\"vertex/pipelines/dummy_pipeline.py\"\nimport kfp.dsl\n\n@kfp.dsl.pipeline()\ndef dummy_pipeline():\n    ...\n</code></pre></p> <p>You can use either <code>.py</code>, <code>.toml</code> or <code>.json</code> files for your config files.</p> <p>You must respect the following folder structure. If you already follow the Vertex Pipelines Starter Kit folder structure, it should be pretty smooth to use this tool:</p> <pre><code>vertex\n\u251c\u2500 configs/\n\u2502  \u2514\u2500 {pipeline_name}\n\u2502     \u2514\u2500 {config_name}.json\n\u2514\u2500 pipelines/\n   \u2514\u2500 {pipeline_name}.py\n</code></pre> <p>About folder structure</p> <p>You must have at least these files. If you need to share some config elements between pipelines, you can have a <code>shared</code> folder in <code>configs</code> and import them in your pipeline configs.</p> <p>If you're following a different folder structure, you can change the default paths in the <code>pyproject.toml</code> file. See Configuration section for more information.</p>"},{"location":"folder_structure/#pipelines","title":"Pipelines","text":"<p>Your file <code>{pipeline_name}.py</code> must contain a function called <code>{pipeline_name}</code> decorated using <code>kfp.dsl.pipeline</code>. In previous versions, the functions / object used to be called <code>pipeline</code> but it was changed to <code>{pipeline_name}</code> to avoid confusion with the <code>kfp.dsl.pipeline</code> decorator.</p> <pre><code># vertex/pipelines/dummy_pipeline.py\nimport kfp.dsl\n\n# New name to avoid confusion with the kfp.dsl.pipeline decorator\n@kfp.dsl.pipeline()\ndef dummy_pipeline():\n    ...\n\n# Old name\n@kfp.dsl.pipeline()\ndef pipeline():\n    ...\n</code></pre>"},{"location":"folder_structure/#configs","title":"Configs","text":"<p>Config file can be either <code>.py</code>, <code>.json</code>, <code>.toml</code> or <code>yaml</code> format. They must be located in the <code>config/{pipeline_name}</code> folder.</p> <p>Why multiple formats?</p> <p><code>.py</code> files are useful to define complex configs (e.g. a list of dicts) while <code>.json</code> / <code>.toml</code> / <code>yaml</code> files are useful to define simple configs (e.g. a string). It also adds flexibility to the user and allows you to use the deployer with almost no migration cost.</p> <p>How to format them?</p> <ul> <li> <p><code>.py</code> files must be valid python files with two important elements:</p> <ul> <li><code>parameter_values</code> to pass arguments to your pipeline</li> <li><code>input_artifacts</code> if you want to retrieve and create input artifacts to your pipeline. See Vertex Documentation for more information.</li> </ul> </li> <li> <p><code>.json</code> files must be valid json files containing only one dict of key: value representing parameter values.</p> </li> <li> <p><code>.toml</code> files must be the same. Please note that TOML sections will be flattened, except for inline tables.     Section names will be joined using <code>\"_\"</code> separator and this is not configurable at the moment.     Example:</p> TOML fileResulting parameter values <pre><code>[modeling]\nmodel_name = \"my-model\"\nparams = { lambda = 0.1 }\n</code></pre> <pre><code>{\n    \"modeling_model_name\": \"my-model\",\n    \"modeling_params\": { \"lambda\": 0.1 }\n}\n</code></pre> </li> <li> <p><code>.yaml</code> files must be valid yaml files containing only one dict of key: value representing parameter values.</p> </li> </ul> Why are sections flattened when using TOML config files? <p>Vertex Pipelines parameter validation and parameter logging to Vertex Experiments are based on the parameter name. If you do not flatten your sections, you'll only be able to validate section names and that they should be of type <code>dict</code>.</p> <p>Not very useful.</p> Why aren't <code>input_artifacts</code> supported in TOML / JSON config files? <p>Because it's low on the priority list. Feel free to open a PR if you want to add it.</p> <p>How to name them?</p> <p><code>{config_name}.py</code> or <code>{config_name}.json</code> or <code>{config_name}.toml</code>. config_name is free but must be unique for a given pipeline.</p>"},{"location":"folder_structure/#settings","title":"Settings","text":"<p>You will also need the following ENV variables, either exported or in a <code>.env</code> file (see example in <code>example.env</code>):</p> <pre><code>PROJECT_ID=YOUR_PROJECT_ID  # GCP Project ID\nGCP_REGION=europe-west1  # GCP Region\n\nGAR_LOCATION=europe-west1  # Google Artifact Registry Location\nGAR_PIPELINES_REPO_ID=YOUR_GAR_KFP_REPO_ID  # Google Artifact Registry Repo ID (KFP format)\n\nVERTEX_STAGING_BUCKET_NAME=YOUR_VERTEX_STAGING_BUCKET_NAME  # GCS Bucket for Vertex Pipelines staging\nVERTEX_SERVICE_ACCOUNT=YOUR_VERTEX_SERVICE_ACCOUNT  # Vertex Pipelines Service Account\n</code></pre> <p>About env files</p> <p>We're using env files and dotenv to load the environment variables. No default value for <code>--env-file</code> argument is provided to ensure that you don't accidentally deploy to the wrong project. An <code>example.env</code> file is provided in this repo. This also allows you to work with multiple environments thanks to env files (<code>test.env</code>, <code>dev.env</code>, <code>prod.env</code>, etc)</p>"},{"location":"setup/","title":"Prerequisites & Setup","text":"TL;DR <p>You need a GCP project ready to use Vertex Pipelines. You'll need to activate the following services:</p> <ul> <li>CloudBuild</li> <li>Artifact Registry</li> <li>Storage</li> <li>Vertex Pipelines</li> </ul> <p>And create some stuff in it.</p>"},{"location":"setup/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ul> <li>Unix-like environment (Linux, macOS, WSL, etc.)</li> <li>Python 3.8 to 3.10</li> <li>Google Cloud SDK</li> <li>A GCP project with Vertex Pipelines enabled</li> </ul>"},{"location":"setup/#setup","title":"\ud83d\udee0\ufe0f Setup","text":"<ol> <li> <p>Setup your GCP environment: <pre><code>export PROJECT_ID=&lt;gcp_project_id&gt;\ngcloud config set project $PROJECT_ID\ngcloud auth login\ngcloud auth application-default login\n</code></pre></p> </li> <li> <p>You need the following APIs to be enabled:</p> </li> <li>Cloud Build API</li> <li>Artifact Registry API</li> <li>Cloud Storage API</li> <li> <p>Vertex AI API <pre><code>gcloud services enable \\\n    cloudbuild.googleapis.com \\\n    artifactregistry.googleapis.com \\\n    storage.googleapis.com \\\n    aiplatform.googleapis.com\n</code></pre></p> </li> <li> <p>Create an artifact registry repository for your base images (Docker format): <pre><code>export GAR_DOCKER_REPO_ID=&lt;your_gar_repo_id_for_images&gt;\nexport GAR_LOCATION=&lt;your_gar_location&gt;\ngcloud artifacts repositories create ${GAR_DOCKER_REPO_ID} \\\n    --location=${GAR_LOCATION} \\\n    --repository-format=docker\n</code></pre></p> </li> <li> <p>Build and upload your base images to the repository. To do so, please follow Google Cloud Build documentation.</p> </li> <li> <p>Create an artifact registry repository for your pipelines (KFP format): <pre><code>export GAR_PIPELINES_REPO_ID=&lt;your_gar_repo_id_for_pipelines&gt;\ngcloud artifacts repositories create ${GAR_PIPELINES_REPO_ID} \\\n    --location=${GAR_LOCATION} \\\n    --repository-format=kfp\n</code></pre></p> </li> <li> <p>Create a GCS bucket for Vertex Pipelines staging: <pre><code>export GCP_REGION=&lt;your_gcp_region&gt;\nexport VERTEX_STAGING_BUCKET_NAME=&lt;your_bucket_name&gt;\ngcloud storage buckets create gs://${VERTEX_STAGING_BUCKET_NAME} --location=${GCP_REGION}\n</code></pre></p> </li> <li> <p>Create a service account for Vertex Pipelines: <pre><code>export VERTEX_SERVICE_ACCOUNT_NAME=foobar\nexport VERTEX_SERVICE_ACCOUNT=\"${VERTEX_SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\"\n\ngcloud iam service-accounts create ${VERTEX_SERVICE_ACCOUNT_NAME}\n\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member=\"serviceAccount:${VERTEX_SERVICE_ACCOUNT}\" \\\n    --role=\"roles/aiplatform.user\"\n\ngcloud storage buckets add-iam-policy-binding gs://${VERTEX_STAGING_BUCKET_NAME} \\\n    --member=\"serviceAccount:${VERTEX_SERVICE_ACCOUNT}\" \\\n    --role=\"roles/storage.objectUser\"\n\ngcloud artifacts repositories add-iam-policy-binding ${GAR_PIPELINES_REPO_ID} \\\n   --location=${GAR_LOCATION} \\\n   --member=\"serviceAccount:${VERTEX_SERVICE_ACCOUNT}\" \\\n   --role=\"roles/artifactregistry.admin\"\n</code></pre></p> </li> </ol>"},{"location":"usage/","title":"Basic Usage","text":"TL;DR <p>Deploy pipeline: <pre><code>vertex-deployer deploy dummy_pipeline \\\n    --compile \\\n    --upload \\\n    --run \\\n    --env-file example.env \\\n    --local-package-path . \\\n    --tags my-tag \\\n    --config-filepath vertex/configs/dummy_pipeline/config_test.json \\\n    --experiment-name my-experiment \\\n    --enable-caching \\\n    --skip-validation\n</code></pre></p> <p>Check pipelines: <pre><code>vertex-deployer check --all\n</code></pre></p>"},{"location":"usage/#cli-deploying-a-pipeline-with-deploy","title":"\ud83d\ude80 CLI: Deploying a Pipeline with <code>deploy</code>","text":"<p>Let's say you defined a pipeline in <code>dummy_pipeline.py</code> and a config file named <code>config_test.json</code>. You can deploy your pipeline using the following command: <pre><code>vertex-deployer deploy dummy_pipeline \\\n    --compile \\\n    --upload \\\n    --run \\\n    --env-file example.env \\\n    --tags my-tag \\\n    --config-filepath vertex/configs/dummy_pipeline/config_test.json \\\n    --experiment-name my-experiment \\\n    --enable-caching \\\n    --skip-validation\n</code></pre></p>"},{"location":"usage/#cli-checking-pipelines-are-valid-with-check","title":"\u2705 CLI: Checking Pipelines are valid with <code>check</code>","text":"<p>To check that your pipelines are valid, you can use the <code>check</code> command. It uses a pydantic model to: - check that your pipeline imports and definition are valid - check that your pipeline can be compiled - check that all configs related to the pipeline are respecting the pipeline definition (using a Pydantic model based on pipeline signature)</p> <p>To validate one or multiple pipeline(s): <pre><code>vertex-deployer check dummy_pipeline &lt;other pipeline name&gt;\n</code></pre></p> <p>To validate all pipelines in the <code>vertex/pipelines</code> folder: <pre><code>vertex-deployer check --all\n</code></pre></p>"},{"location":"usage/#cli-other-commands","title":"\ud83d\udee0\ufe0f CLI: Other commands","text":""},{"location":"usage/#config","title":"<code>config</code>","text":"<p>You can check your <code>vertex-deployer</code> configuration options using the <code>config</code> command. Fields set in <code>pyproject.toml</code> will overwrite default values and will be displayed differently: <pre><code>vertex-deployer config --all\n</code></pre></p>"},{"location":"usage/#create","title":"<code>create</code>","text":"<p>You can create all files needed for a pipeline using the <code>create</code> command: <pre><code>vertex-deployer create my_new_pipeline --config-type py\n</code></pre></p> <p>This will create a <code>my_new_pipeline.py</code> file in the <code>vertex/pipelines</code> folder and a <code>vertex/config/my_new_pipeline/</code> folder with multiple config files in it.</p>"},{"location":"usage/#init","title":"<code>init</code>","text":"<p>To initialize the deployer with default settings and folder structure, use the <code>init</code> command: <pre><code>vertex-deployer init\n</code></pre></p> <pre><code>$ vertex-deployer init\nWelcome to Vertex Deployer!\nThis command will help you getting fired up.\nDo you want to configure the deployer? [y/n]: n\nDo you want to build default folder structure [y/n]: n\nDo you want to create a pipeline? [y/n]: n\nAll done \u2728\n</code></pre>"},{"location":"usage/#list","title":"<code>list</code>","text":"<p>You can list all pipelines in the <code>vertex/pipelines</code> folder using the <code>list</code> command: <pre><code>vertex-deployer list --with-configs\n</code></pre></p>"},{"location":"usage/#cli-options","title":"\ud83c\udf6d CLI: Options","text":"<pre><code>vertex-deployer --help\n</code></pre> <p>To see package version: <pre><code>vertex-deployer --version\n</code></pre></p> <p>To adapt log level, use the <code>--log-level</code> option. Default is <code>INFO</code>. <pre><code>vertex-deployer --log-level DEBUG deploy ...\n</code></pre></p>"},{"location":"advanced/advanced_user_guide/","title":"Vertex DevOps","text":"<p>Tip</p> <p>Add code provided in this page is available in the repo example.</p>"},{"location":"advanced/advanced_user_guide/#dev-compile-and-run-to-fasten-your-dev-cycle","title":"\ud83d\udcbb Dev: Compile and run to fasten your dev cycle","text":"<p>When developing a vertex pipeline locally, you may want to iterate quickly. The process is often the following:</p> <ol> <li>write new code and test its integration in the pipeline workflow code in a notebook</li> <li>script this new code in vertex/lib/</li> <li>modify associated component(s) in <code>vertex/components</code> and pipelines in <code>vertex/pipelines</code></li> <li>run the pipeline with dev settings to test the new code on Vertex</li> </ol> <p>The latest may include:</p> <ul> <li>rebuilding the base image</li> <li>compiling the pipeline</li> <li>running the pipeline</li> </ul>"},{"location":"advanced/advanced_user_guide/#build-base-image","title":"Build base image","text":"<p>You can use this generic Dockerfile:</p> <pre><code>FROM python:3.10-slim-buster\n\nARG PROJECT_ID\nENV PROJECT_ID=${PROJECT_ID}\n\nCOPY requirements.txt .\nRUN python3 -m pip install --upgrade pip\nRUN python3 -m pip install -r requirements.txt\n\nCOPY vertex .\nENV PYTHONPATH \"${PYTHONPATH}:.\"\n</code></pre> <p>Then build it with docker or Cloud Build. For the latest, here is a sample cloudbuild.yaml:</p> <pre><code># This config file is meant to be used from a local dev machine to submit a vertex base image build to Cloud Build.\n# This generic image will then be used in all the Vertex components of your pipeline.\n\nsteps:\n  # Build base image\n  - name: 'gcr.io/cloud-builders/docker'\n    args: [\n      'build',\n      '-t', '${_GAR_IMAGE_PATH}',\n      '-f', 'vertex/deployment/Dockerfile',\n      '--build-arg', 'PROJECT_ID=${PROJECT_ID}',\n      '--build-arg', 'GCP_REGION=${_GCP_REGION}',\n      '--build-arg', 'GAR_LOCATION=${_GAR_LOCATION}',\n      '--build-arg', 'GAR_PIPELINES_REPO_ID=${_GAR_PIPELINES_REPO_ID}',\n      '--build-arg', 'VERTEX_STAGING_BUCKET_NAME=${_VERTEX_STAGING_BUCKET_NAME}',\n      '--build-arg', 'VERTEX_SERVICE_ACCOUNT=${_VERTEX_SERVICE_ACCOUNT}',\n      '.',\n    ]\n    id: build-base-image\n\nsubstitutions:\n  _GAR_IMAGE_PATH: '${_GAR_LOCATION}-docker.pkg.dev/${PROJECT_ID}/${_GAR_DOCKER_REPO_ID}/${_GAR_VERTEX_BASE_IMAGE_NAME}:${_TAG}'\n  _GCP_REGION: '${GCP_REGION}'\n  _GAR_PIPELINES_REPO_ID: '${GAR_PIPELINES_REPO_ID}'\n  _VERTEX_STAGING_BUCKET_NAME: '${VERTEX_STAGING_BUCKET_NAME}'\n  _VERTEX_SERVICE_ACCOUNT: '${VERTEX_SERVICE_ACCOUNT}'\n\noptions:\n  logging: CLOUD_LOGGING_ONLY\n  dynamic_substitutions: true\n\nimages:\n- '${_GAR_IMAGE_PATH}'\n\ntags:\n  - vertex-${_GAR_DOCKER_REPO_ID}-base-image-local-${_TAG}\n</code></pre> <p>Then you can trigger the build manually using this make command:</p> <pre><code>export $(cat .env | xargs)\nmake build-base-image\n</code></pre> <p>This command includes the following: <pre><code>.PHONY: build-base-image\nbuild-base-image:\n    @gcloud builds submit --config ./vertex/deployment/cloudbuild_local.yaml \\\n    --substitutions=_GCP_REGION=${GCP_REGION},_GAR_LOCATION=${GAR_LOCATION},_GAR_DOCKER_REPO_ID=${GAR_DOCKER_REPO_ID},_GAR_VERTEX_BASE_IMAGE_NAME=${GAR_VERTEX_BASE_IMAGE_NAME},_TAG=${TAG},_GAR_PIPELINES_REPO_ID=${GAR_PIPELINES_REPO_ID},_VERTEX_STAGING_BUCKET_NAME=${VERTEX_STAGING_BUCKET_NAME},_VERTEX_SERVICE_ACCOUNT=${VERTEX_SERVICE_ACCOUNT}\n</code></pre></p>"},{"location":"advanced/advanced_user_guide/#compile-and-run","title":"Compile and run","text":"<p>Now that you have a base image, you can compile your pipeline and trigger a run that will use the latest version of your docker base image</p> <pre><code>vertex-deployer deploy --compile --run --env-file .env --config-name my_config.json --skip-validation\n</code></pre>"},{"location":"advanced/advanced_user_guide/#ci-check-your-pipelines-and-config-integrity","title":"\u2705 CI: Check your pipelines and config integrity","text":""},{"location":"advanced/advanced_user_guide/#check-your-pipelines-locally","title":"Check your pipelines locally","text":"<p>You can check pipelines integrity and config integrity using the following command:</p> <pre><code>vertex-deployer check --all\n</code></pre> <p>To check a specific pipeline: <pre><code>vertex-deployer check my_pipeline\n</code></pre></p>"},{"location":"advanced/advanced_user_guide/#add-to-ci","title":"Add to CI","text":"<p>You can add a github workflow checking your pipelines integrity using the following file: <pre><code>name: Check pipelines\n\non:\n  pull_request:\n    branches:\n      - '*'\n  workflow_call:\n\nenv:\n  PYTHON_VERSION: \"3.10\"\n\njobs:\n  check-pipelines:\n    name: Check Vertex Pipelines\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python ${{ env.PYTHON_VERSION }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n          cache: 'pip'\n\n      - name: Install requirements\n        run: |\n          python3 -m pip install -r requirements.txt\n\n      - name: Check pipelines\n        run: |\n          export PYTHONPATH=.\n          vertex-deployer check --all\n</code></pre></p>"},{"location":"advanced/advanced_user_guide/#add-to-pre-commit-hooks","title":"Add to pre-commit hooks","text":"<p>You can add a pre-commit hook checking your pipelines integrity using a local hook: <pre><code>repos:\n  - repo: local\n    hooks:\n    - id: vertex-deployer-check\n      name: check pipelines\n      entry: vertex-deployer check -a\n      pass_filenames: false\n      language: system\n</code></pre></p>"},{"location":"advanced/advanced_user_guide/#cd-deploy-your-pipelines-in-a-standardized-manner","title":"\ud83d\ude80 CD: Deploy your pipelines in a standardized manner","text":"<p>Once you have a valid pipeline, you want to deploy it on Vertex. To automate deployment when merging to <code>develop</code> or <code>main</code>, you have multiple options.</p> <ul> <li>use CloudBuild and CloudBuild triggers</li> <li>use Github Action to trigger CloudBuild job</li> <li>\ud83d\udea7 use Github Action only</li> </ul> <p>Note</p> <p>To use cloudbuild for CD, please update you Dockerfile with all these arguments. This will allow you to use <code>vertex-deployer</code> from your base image in CloudBuild.</p> <pre><code>FROM python:3.10-slim-buster\n\nARG PROJECT_ID\nARG GCP_REGION\nARG GAR_LOCATION\nARG GAR_PIPELINES_REPO_ID\nARG VERTEX_STAGING_BUCKET_NAME\nARG VERTEX_SERVICE_ACCOUNT\n\nENV PROJECT_ID=${PROJECT_ID}\nENV GCP_REGION=${GCP_REGION}\nENV GAR_LOCATION=${GAR_LOCATION}\nENV GAR_PIPELINES_REPO_ID=${GAR_PIPELINES_REPO_ID}\nENV VERTEX_STAGING_BUCKET_NAME=${VERTEX_STAGING_BUCKET_NAME}\nENV VERTEX_SERVICE_ACCOUNT=${VERTEX_SERVICE_ACCOUNT}\n\nCOPY requirements.txt .\nRUN python3 -m pip install --upgrade pip\nRUN python3 -m pip install -r requirements.txt\n\nCOPY vertex .\nENV PYTHONPATH \"${PYTHONPATH}:.\"\n</code></pre>"},{"location":"advanced/advanced_user_guide/#cloudbuild","title":"CloudBuild","text":"<p>You can use the following cloudbuild.yaml to trigger a deployment on Vertex when merging to <code>develop</code> or <code>main</code>:</p> <pre><code>steps:\n  # Build base image\n  - name: 'gcr.io/cloud-builders/docker'\n    args: [\n      'build',\n      '-t', '${_GAR_IMAGE_PATH}',\n      '-f', 'vertex/deployment/Dockerfile',\n      '--build-arg', 'PROJECT_ID=${PROJECT_ID}',\n      '--build-arg', 'GCP_REGION=${_GCP_REGION}',\n      '--build-arg', 'GAR_LOCATION=${_GAR_LOCATION}',\n      '--build-arg', 'GAR_PIPELINES_REPO_ID=${_GAR_PIPELINES_REPO_ID}',\n      '--build-arg', 'VERTEX_STAGING_BUCKET_NAME=${_VERTEX_STAGING_BUCKET_NAME}',\n      '--build-arg', 'VERTEX_SERVICE_ACCOUNT=${_VERTEX_SERVICE_ACCOUNT}',\n      '.',\n    ]\n    id: build-base-image\n\n  # schedule pipeline: compile, upload, schedule\n  - name: '${_GAR_IMAGE_PATH}'\n    entrypoint: 'bash'\n    args:\n      - '-c'\n      - |\n        vertex-deployer -log DEBUG deploy dummy_pipeline \\\n          --compile \\\n          --upload \\\n          --run \\\n          --enable-caching \\\n          --config-name config_test.json \\\n          --tags ${_TAG} \\\n          --schedule --delete-last-schedule --cron '*-*-19-*-*'\n\n    dir: '.'\n    id: schedule-dummy-pipeline\n    waitFor: ['build-base-image']\n\nsubstitutions:\n  _GAR_IMAGE_PATH: '${_GAR_LOCATION}-docker.pkg.dev/${PROJECT_ID}/${_GAR_DOCKER_REPO_ID}/${_GAR_VERTEX_BASE_IMAGE_NAME}:${_TAG}'\n\noptions:\n  logging: CLOUD_LOGGING_ONLY\n  dynamic_substitutions: true\n\nimages:\n- '${_GAR_IMAGE_PATH}'\n\ntags:\n  - vertex-${_GAR_DOCKER_REPO_ID}-deployment-example-${_TAG}\n</code></pre>"},{"location":"advanced/advanced_user_guide/#use-cloudbuild-trigger-preferred-option","title":"Use CloudBuild trigger [PREFERRED OPTION]","text":"<p>Then, you'll need to link your repo to CloudBuild and create a trigger for each branch you want to deploy on Vertex. The documentation to link your repo is available here.</p> <p>Then, you can create create a trigger using this make command:</p> <pre><code>export $(cat .env | xargs)\nmake create-trigger-cd\n</code></pre> <p>This command includes the following: <pre><code>.PHONY: create-trigger-cd\ncreate-trigger-cd:\n    @gcloud builds triggers create github \\\n    --repo-owner=\"artefactory\" \\\n    --repo-name=\"test-vertex-deployer\" \\\n    --name=\"test-vertex-deployer-trigger\" \\\n    --branch-pattern=\"main\" \\\n    --build-config=./vertex/deployment/cloudbuild_cd.yaml \\\n    --project=${PROJECT_ID} \\\n    --substitutions=_GCP_REGION=${GCP_REGION},_GAR_LOCATION=${GAR_LOCATION},_GAR_DOCKER_REPO_ID=${GAR_DOCKER_REPO_ID},_GAR_VERTEX_BASE_IMAGE_NAME=${GAR_VERTEX_BASE_IMAGE_NAME},_TAG=${TAG},_GAR_PIPELINES_REPO_ID=${GAR_PIPELINES_REPO_ID},_VERTEX_STAGING_BUCKET_NAME=${VERTEX_STAGING_BUCKET_NAME},_VERTEX_SERVICE_ACCOUNT=${VERTEX_SERVICE_ACCOUNT}\n</code></pre></p>"},{"location":"advanced/advanced_user_guide/#github-action-cloudbuild","title":"Github Action + CloudBuild","text":"<p>You can also use Github Action to trigger CloudBuild job. You'll need to setup GCp authentication from your repo using Workload Identity Federation.</p> <pre><code># This workflow deploys a pipeline to Vertex AI Pipelines.\n#\n# Workflow Steps:\n#\n# 1. Check pipelines and config integrity\n# 2. Authenticate to Google Cloud using Workload Identity Federation (WIF)\n# 3. Submit cloud build job to build docker image and deploy pipeline\n#\n# For more details on setting up Workload Identity Federation for GitHub, visit https://github.com/google-github-actions/auth#setting-up-workload-identity-federation\n#\n# Your WIF service account must have the following IAM roles:\n# - roles/artifactregistry.writer\n# - roles/storage.admin\n# - roles/cloudbuild.builds.builder\n\nname: Deploy pipelines\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PYTHON_VERSION: \"3.10\"\n  PROJECT_ID: \"my-project\"\n  GCP_REGION: \"europe-west1\"\n  TAG: \"latest\"\n  # Google Artifact Registry\n  GAR_LOCATION: \"europe-west1\"\n  GAR_DOCKER_REPO_ID: \"demo-docker\"\n  GAR_PIPELINES_REPO_ID: \"test-pipelines\"\n  GAR_VERTEX_BASE_IMAGE_NAME: \"base-image\"\n  # Vertex AI\n  VERTEX_STAGING_BUCKET_NAME: \"my-project-vertex-staging\"\n  VERTEX_SERVICE_ACCOUNT: \"my-service-account@my-project.iam.gserviceaccount.com\"\n\njobs:\n  check-pipelines:\n      name: Check Pipelines\n      uses: ./.github/workflows/check_pipelines.yaml\n\n  deploy-pipelines:\n    name: Deploy pipelines\n    needs: check-pipelines\n    runs-on: ubuntu-latest\n    concurrency: deploy-pipelines\n    permissions:\n      id-token: write\n      contents: write\n\n    steps:\n    - uses: actions/checkout@v3\n      with:\n        fetch-depth: 0\n\n    - name: Set up Python ${{ env.PYTHON_VERSION }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ env.PYTHON_VERSION }}\n        cache: 'pip'\n\n    - name: Install requirements\n      run: |\n        python3 -m pip install --upgrade pip\n        python3 -m pip install -r requirements.txt\n\n    - name: 'Authenticate to Google Cloud'\n      uses: 'google-github-actions/auth@v1'\n      with:\n        token_format: 'access_token'\n        workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n        service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n    - name: Trigger Cloud Build\n      run: |\n        export PROJECT_ID=vertex-deployer-sandbox-3a8a\n        make deploy-pipeline\n</code></pre>"},{"location":"advanced/advanced_user_guide/#github-action-only","title":"\ud83d\udea7 Github Action only","text":"<p>Warning</p> <p>This is a work in progress. Please use CloudBuild for CD. Docker build and push to GCR example is not yet implemented.</p>"},{"location":"advanced/configuration/","title":"Understand settings and configurations","text":""},{"location":"advanced/configuration/#three-types-of-settings","title":"Three types of settings","text":"<p>There are three types of settings in the project:</p> <ul> <li>Vertex Deployer configuration: settings of the deployer itself, which is used to deploy the project.</li> <li>Pipelines config files: configuration of the pipelines, in TOML/JSON/Python format. These files are the arguments to your pipelines.</li> <li>Vertex deployment settings: used only by the deploy command, it consists of a few env variables to declare / add to <code>.env</code> file to deploy a pipeline, such as <code>PROJECT_ID</code>, <code>GCP_REGION</code>, <code>VERTEX_STAGING_BUCKET_NAME</code>, etc.</li> </ul>"},{"location":"advanced/configuration/#vertex-deployer-configuration","title":"Vertex Deployer configuration","text":"<p>You can override default options for specific CLI commands in the pyproject.toml file, under the <code>[tool.vertex_deployer]</code> section. You can also override global deployer options such as logging level, or pipelines / config root path to better fit your repo structure.</p> pyproject.toml<pre><code>[tool.vertex_deployer]\nlog-level = \"INFO\"\npipelines-root-path = \"./vertex/pipelines\"\nconfig-root-path = \"./configs\"\n\n[tool.vertex_deployer.deploy]\nenable-cache = true\nenv-file = \"example.env\"\ncompile = true\nupload = true\nrun = true\ntags = [\"my-tag\"]\nexperiment-name = \"my-experiment\"\nlocal-package-path = \".\"\nconfig-filepath = \"vertex/configs/dummy_pipeline/config_test.json\"\n</code></pre>"},{"location":"advanced/configuration/#pipelines-config-files","title":"Pipelines config files","text":"<p>Config files for pipelines can be in <code>.py</code>, <code>.json</code>, or <code>.toml</code> format and must be located in the <code>config/{pipeline_name}</code> folder. The choice of format depends on the complexity and requirements of the configuration. Python files allow for complex configurations and dynamic values, while JSON and TOML files are more suitable for static and simple configurations.</p> <p>For example, you have here the same config file in the three formats:</p> JSONTOMLYAMLPython vertex/configs/dummy_pipeline/config_test.json<pre><code>{\n    \"model_name\": \"my-model\",\n    \"default_params\": {\n        \"lambda\": 0.1,\n        \"alpha\": \"hello world\"\n    },\n    \"grid_search\": {\n        \"lambda\": [0.1, 0.2, 0.3],\n        \"alpha\": [\"hello world\", \"goodbye world\"],\n        \"cv\": 3\n    }\n}\n</code></pre> <p>JSON config files are the simplest and most readable, but they are also the most limited.</p> <p>They do not allow for dynamic values or complex configurations.</p> <p>They are the default.</p> vertex/configs/dummy_pipeline/config_test.toml<pre><code>[modeling]\nmodel_name = \"my-model\"\ndefault_params = { lambda = 0.1 , alpha = \"hello world\"}\n\n[modeling.grid_search]\nlambda = [0.1, 0.2, 0.3]\nalpha = [\"hello world\", \"goodbye world\"]\ncv = 3\n</code></pre> <p>TOML config files are more flexible than JSON files, but they are also more verbose.</p> <p>They allow structuring the config file in sections, which can be useful for complex configurations.</p> <p>Then, these sections are flattened, except for inline dicts, leading to slightly different parameter names (e.g., <code>modeling_grid_search_lambda</code> instead of <code>lambda</code>).</p> vertex/configs/dummy_pipeline/config_test.yaml<pre><code>model_name: my-model\ndefault_params:\n  lambda: 0.1\n  alpha: hello world\ngrid_search:\n  lambda:\n    - 0.1\n    - 0.2\n    - 0.3\n  alpha:\n    - hello world\n    - goodbye world\n  cv: 3\n</code></pre> <p>YAML config files are similar to TOML files in terms of flexibility and verbosity.</p> <p>They are more human-readable than TOML files, but they are also more error-prone due to indentation.</p> vertex/configs/dummy_pipeline/config_test.py<pre><code>parameter_values = {\n    \"model_name\": \"my-model\",\n    \"default_params\": {\n        \"lambda\": 0.1,\n        \"alpha\": \"hello world\"\n    },\n    \"grid_search\": {\n        \"lambda\": [0.1, 0.2, 0.3],\n        \"alpha\": [\"hello world\", \"goodbye world\"],\n        \"cv\": 3\n    }\n}\n\ninput_artifacts = {  # Only available in Python config files\n    \"artifact1\": \"gs://bucket/path/to/artifact1\"\n}\n</code></pre> <p>Python config files are the most flexible, as they allow for dynamic values and complex configurations.</p> <p>They are also the only format that allows for the use of input artifacts.</p> <p>However, they are also the most verbose and require more boilerplate code.</p>"},{"location":"advanced/configuration/#vertex-deployment-settings","title":"Vertex deployment settings","text":"<p>The deployment settings are environment variables that configure the deployment environment for Vertex Pipelines. These variables include the GCP project ID, region, and other settings related to the Google Cloud resources used by Vertex Pipelines.</p> <p>These settings can be specified in an <code>.env</code> file or exported as environment variables. An example <code>.env</code> file might look like this: <pre><code>PROJECT_ID=your-gcp-project-id\nGCP_REGION=europe-west1\nGAR_LOCATION=europe-west1\nGAR_PIPELINES_REPO_ID=your-gar-kfp-repo-id\nVERTEX_STAGING_BUCKET_NAME=your-vertex-staging-bucket-name\nVERTEX_SERVICE_ACCOUNT=your-vertex-service-account\n</code></pre></p> <p>It is important to ensure that these settings are correctly configured before deploying a pipeline, as they will affect where and how the pipeline is executed.</p>"},{"location":"api/pipeline_checks/","title":"pipeline_checks","text":""},{"location":"api/pipeline_checks/#deployer.pipeline_checks.Pipelines","title":"Pipelines","text":"<p>               Bases: <code>CustomBaseModel</code></p> <p>Model to validate multiple pipelines at once</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>class Pipelines(CustomBaseModel):\n    \"\"\"Model to validate multiple pipelines at once\"\"\"\n\n    pipelines: Dict[str, Pipeline]\n\n    @model_validator(mode=\"wrap\")\n    def _init_remove_temp_directory(self, handler: ModelWrapValidatorHandler) -&gt; Any:\n        \"\"\"Create and remove temporary directory\"\"\"\n        Path(TEMP_LOCAL_PACKAGE_PATH).mkdir(exist_ok=True)\n\n        try:\n            validated_self = handler(self)\n        except ValidationError as e:\n            raise e\n        finally:\n            shutil.rmtree(TEMP_LOCAL_PACKAGE_PATH)\n\n        return validated_self\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.Pipeline","title":"Pipeline","text":"<p>               Bases: <code>CustomBaseModel</code></p> <p>Validation of one pipeline and its configs</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>class Pipeline(CustomBaseModel):\n    \"\"\"Validation of one pipeline and its configs\"\"\"\n\n    pipeline_name: str\n    config_paths: Annotated[List[Path], Field(validate_default=True)] = None\n    pipelines_root_path: Path\n    configs_root_path: Path\n    configs: Optional[Dict[str, ConfigDynamicModel]] = None  # Optional because populated after\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def populate_config_names(cls, data: Any) -&gt; Any:\n        \"\"\"Populate config names before validation\"\"\"\n        if data.get(\"config_paths\") is None:\n            data[\"config_paths\"] = list_config_filepaths(\n                str(data[\"configs_root_path\"]), data[\"pipeline_name\"]\n            )\n        return data\n\n    @computed_field\n    @property\n    def pipeline(self) -&gt; graph_component.GraphComponent:\n        \"\"\"Import pipeline\"\"\"\n        if getattr(self, \"_pipeline\", None) is None:\n            with DisableLogger(\"deployer.utils.utils\"):\n                self._pipeline = import_pipeline_from_dir(\n                    str(self.pipelines_root_path), self.pipeline_name\n                )\n        return self._pipeline\n\n    @model_validator(mode=\"after\")\n    def import_pipeline(self):\n        \"\"\"Validate that the pipeline can be imported by calling pipeline computed field\"\"\"\n        logger.debug(f\"Importing pipeline {self.pipeline_name}\")\n        try:\n            _ = self.pipeline\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ValueError(f\"Pipeline import failed: {e}\") from e\n        return self\n\n    @model_validator(mode=\"after\")\n    def compile_pipeline(self):\n        \"\"\"Validate that the pipeline can be compiled\"\"\"\n        logger.debug(f\"Compiling pipeline {self.pipeline_name}\")\n        try:\n            with DisableLogger(\"deployer.pipeline_deployer\"):\n                VertexPipelineDeployer(\n                    pipeline_name=self.pipeline_name,\n                    pipeline_func=self.pipeline,\n                    local_package_path=TEMP_LOCAL_PACKAGE_PATH,\n                ).compile()\n        except Exception as e:\n            raise ValueError(f\"Pipeline compilation failed: {e.__repr__()}\")  # noqa: B904\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_configs(self, info: ValidationInfo):\n        \"\"\"Validate configs against pipeline parameters definition\"\"\"\n        logger.debug(f\"Validating configs for pipeline {self.pipeline_name}\")\n        pipelines_dynamic_model = create_model_from_func(\n            self.pipeline.pipeline_func,\n            type_converter=_convert_artifact_type_to_str,\n            exclude_defaults=info.context.get(\"raise_for_defaults\", False),\n        )\n        config_model = ConfigsDynamicModel[pipelines_dynamic_model]\n        self.configs = config_model.model_validate(\n            {\"configs\": {x.name: {\"config_path\": x} for x in self.config_paths}}\n        )\n        return self\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.Pipeline.populate_config_names","title":"populate_config_names  <code>classmethod</code>","text":"<pre><code>populate_config_names(data: Any) -&gt; Any\n</code></pre> <p>Populate config names before validation</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef populate_config_names(cls, data: Any) -&gt; Any:\n    \"\"\"Populate config names before validation\"\"\"\n    if data.get(\"config_paths\") is None:\n        data[\"config_paths\"] = list_config_filepaths(\n            str(data[\"configs_root_path\"]), data[\"pipeline_name\"]\n        )\n    return data\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.Pipeline.pipeline","title":"pipeline  <code>property</code>","text":"<pre><code>pipeline: GraphComponent\n</code></pre> <p>Import pipeline</p>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.Pipeline.import_pipeline","title":"import_pipeline","text":"<pre><code>import_pipeline()\n</code></pre> <p>Validate that the pipeline can be imported by calling pipeline computed field</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>@model_validator(mode=\"after\")\ndef import_pipeline(self):\n    \"\"\"Validate that the pipeline can be imported by calling pipeline computed field\"\"\"\n    logger.debug(f\"Importing pipeline {self.pipeline_name}\")\n    try:\n        _ = self.pipeline\n    except (ImportError, ModuleNotFoundError) as e:\n        raise ValueError(f\"Pipeline import failed: {e}\") from e\n    return self\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.Pipeline.compile_pipeline","title":"compile_pipeline","text":"<pre><code>compile_pipeline()\n</code></pre> <p>Validate that the pipeline can be compiled</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>@model_validator(mode=\"after\")\ndef compile_pipeline(self):\n    \"\"\"Validate that the pipeline can be compiled\"\"\"\n    logger.debug(f\"Compiling pipeline {self.pipeline_name}\")\n    try:\n        with DisableLogger(\"deployer.pipeline_deployer\"):\n            VertexPipelineDeployer(\n                pipeline_name=self.pipeline_name,\n                pipeline_func=self.pipeline,\n                local_package_path=TEMP_LOCAL_PACKAGE_PATH,\n            ).compile()\n    except Exception as e:\n        raise ValueError(f\"Pipeline compilation failed: {e.__repr__()}\")  # noqa: B904\n    return self\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.Pipeline.validate_configs","title":"validate_configs","text":"<pre><code>validate_configs(info: ValidationInfo)\n</code></pre> <p>Validate configs against pipeline parameters definition</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_configs(self, info: ValidationInfo):\n    \"\"\"Validate configs against pipeline parameters definition\"\"\"\n    logger.debug(f\"Validating configs for pipeline {self.pipeline_name}\")\n    pipelines_dynamic_model = create_model_from_func(\n        self.pipeline.pipeline_func,\n        type_converter=_convert_artifact_type_to_str,\n        exclude_defaults=info.context.get(\"raise_for_defaults\", False),\n    )\n    config_model = ConfigsDynamicModel[pipelines_dynamic_model]\n    self.configs = config_model.model_validate(\n        {\"configs\": {x.name: {\"config_path\": x} for x in self.config_paths}}\n    )\n    return self\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.ConfigsDynamicModel","title":"ConfigsDynamicModel","text":"<p>               Bases: <code>CustomBaseModel</code>, <code>Generic[PipelineConfigT]</code></p> <p>Model used to generate checks for configs based on pipeline dynamic model</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>class ConfigsDynamicModel(CustomBaseModel, Generic[PipelineConfigT]):\n    \"\"\"Model used to generate checks for configs based on pipeline dynamic model\"\"\"\n\n    configs: Dict[str, ConfigDynamicModel[PipelineConfigT]]\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.ConfigDynamicModel","title":"ConfigDynamicModel","text":"<p>               Bases: <code>CustomBaseModel</code>, <code>Generic[PipelineConfigT]</code></p> <p>Model used to generate checks for configs based on pipeline dynamic model</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>class ConfigDynamicModel(CustomBaseModel, Generic[PipelineConfigT]):\n    \"\"\"Model used to generate checks for configs based on pipeline dynamic model\"\"\"\n\n    config_path: Path\n    config: PipelineConfigT\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def load_config_if_empty(cls, data: Any) -&gt; Any:\n        \"\"\"Load config if it is empty\"\"\"\n        if data.get(\"config\") is None:\n            try:\n                parameter_values, input_artifacts = load_config(data[\"config_path\"])\n            except BadConfigError as e:\n                raise PydanticCustomError(\"BadConfigError\", str(e)) from e\n            data[\"config\"] = {**(parameter_values or {}), **(input_artifacts or {})}\n        return data\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks.ConfigDynamicModel.load_config_if_empty","title":"load_config_if_empty  <code>classmethod</code>","text":"<pre><code>load_config_if_empty(data: Any) -&gt; Any\n</code></pre> <p>Load config if it is empty</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>@model_validator(mode=\"before\")\n@classmethod\ndef load_config_if_empty(cls, data: Any) -&gt; Any:\n    \"\"\"Load config if it is empty\"\"\"\n    if data.get(\"config\") is None:\n        try:\n            parameter_values, input_artifacts = load_config(data[\"config_path\"])\n        except BadConfigError as e:\n            raise PydanticCustomError(\"BadConfigError\", str(e)) from e\n        data[\"config\"] = {**(parameter_values or {}), **(input_artifacts or {})}\n    return data\n</code></pre>"},{"location":"api/pipeline_checks/#deployer.pipeline_checks._convert_artifact_type_to_str","title":"_convert_artifact_type_to_str","text":"<pre><code>_convert_artifact_type_to_str(annotation: type) -&gt; type\n</code></pre> <p>Convert a kfp.dsl.Artifact type to a string.</p> <p>This is mandatory for type checking, as kfp.dsl.Artifact types should be passed as strings to VertexAI. See https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob for details.</p> Source code in <code>deployer/pipeline_checks.py</code> <pre><code>def _convert_artifact_type_to_str(annotation: type) -&gt; type:\n    \"\"\"Convert a kfp.dsl.Artifact type to a string.\n\n    This is mandatory for type checking, as kfp.dsl.Artifact types should be passed as strings\n    to VertexAI. See &lt;https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob&gt;\n    for details.\n    \"\"\"\n    if isinstance(annotation, _AnnotatedAlias):\n        if issubclass(annotation.__origin__, kfp.dsl.Artifact):\n            return str\n    return annotation\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#deployer.utils.models","title":"deployer.utils.models","text":""},{"location":"api/utils/#deployer.utils.models.create_model_from_func","title":"create_model_from_func","text":"<pre><code>create_model_from_func(\n    func: Callable,\n    model_name: Optional[str] = None,\n    type_converter: Optional[TypeConverterType] = None,\n    exclude_defaults: bool = False,\n) -&gt; CustomBaseModel\n</code></pre> <p>Create a Pydantic model from pipeline parameters.</p> Source code in <code>deployer/utils/models.py</code> <pre><code>def create_model_from_func(\n    func: Callable,\n    model_name: Optional[str] = None,\n    type_converter: Optional[TypeConverterType] = None,\n    exclude_defaults: bool = False,\n) -&gt; CustomBaseModel:\n    \"\"\"Create a Pydantic model from pipeline parameters.\"\"\"\n    if model_name is None:\n        model_name = func.__name__\n\n    if type_converter is None:\n        type_converter = _dummy_type_converter\n\n    func_signature = signature(func)\n\n    func_typing = {\n        p.name: (\n            type_converter(p.annotation),\n            ... if (exclude_defaults or p.default == Parameter.empty) else p.default,\n        )\n        for p in func_signature.parameters.values()\n    }\n\n    func_model = create_model(\n        model_name,\n        __base__=CustomBaseModel,\n        **func_typing,\n    )\n\n    return func_model\n</code></pre>"},{"location":"api/utils/#deployer.utils.utils","title":"deployer.utils.utils","text":""},{"location":"api/utils/#deployer.utils.utils.make_enum_from_python_package_dir","title":"make_enum_from_python_package_dir","text":"<pre><code>make_enum_from_python_package_dir(\n    dir_path: Path, raise_if_not_found: bool = False\n) -&gt; Enum\n</code></pre> <p>Create an Enum of file names without extention from a directory of python modules.</p> Source code in <code>deployer/utils/utils.py</code> <pre><code>def make_enum_from_python_package_dir(dir_path: Path, raise_if_not_found: bool = False) -&gt; Enum:\n    \"\"\"Create an Enum of file names without extention from a directory of python modules.\"\"\"\n    dir_path_ = Path(dir_path)\n    if raise_if_not_found and not dir_path_.exists():\n        raise FileNotFoundError(f\"Directory {dir_path_} not found.\")\n    file_paths = dir_path_.glob(\"*.py\")\n    enum_dict = {x.stem: x.stem for x in file_paths if x.stem != \"__init__\"}\n    file_names_enum = Enum(dir_path_.stem, enum_dict)\n    return file_names_enum\n</code></pre>"},{"location":"api/utils/#deployer.utils.utils.import_pipeline_from_dir","title":"import_pipeline_from_dir","text":"<pre><code>import_pipeline_from_dir(\n    dirpath: Path, pipeline_name: str\n) -&gt; GraphComponentType\n</code></pre> <p>Import a pipeline from a directory.</p> Source code in <code>deployer/utils/utils.py</code> <pre><code>def import_pipeline_from_dir(dirpath: Path, pipeline_name: str) -&gt; GraphComponentType:\n    \"\"\"Import a pipeline from a directory.\"\"\"\n    dirpath_ = Path(dirpath).resolve().relative_to(Path.cwd())\n    parent_module = \".\".join(dirpath_.parts)\n    module_import_path = f\"{parent_module}.{pipeline_name}\"  # used with import statements\n    module_folder_path = dirpath_ / f\"{pipeline_name}.py\"  # used as a path to a file\n\n    try:\n        pipeline_module = importlib.import_module(module_import_path)\n    except ModuleNotFoundError as e:\n        raise e\n    except Exception as e:\n        raise ImportError(\n            f\"Error while importing pipeline from {module_import_path}: \\n    {type(e).__name__}:\"\n            f\"{e} \\n\\nPotential sources of error:\\n\"\n            f\"{filter_lines_from(e.__traceback__, module_folder_path)}\"\n        ) from e\n\n    try:\n        pipeline: GraphComponentType = getattr(pipeline_module, pipeline_name, None)\n        if pipeline is None:\n            pipeline = pipeline_module.pipeline\n            warnings.warn(\n                f\"Pipeline in `{module_import_path}` is named `pipeline` instead of \"\n                f\"`{pipeline_name}`. This is deprecated and will be removed in a future version. \"\n                f\"Please rename your pipeline to `{pipeline_name}`.\",\n                FutureWarning,\n                stacklevel=1,\n            )\n    except AttributeError as e:\n        raise ImportError(\n            f\"Pipeline object not found in `{module_import_path}`. \"\n            \"Please check that the pipeline is correctly defined and named.\"\n            f\"It should be named `{pipeline_name}` or `pipeline` (deprecated).\"\n        ) from e\n\n    logger.debug(f\"Pipeline {module_import_path} imported successfully.\")\n\n    return pipeline\n</code></pre>"},{"location":"api/utils/#deployer.utils.config","title":"deployer.utils.config","text":""},{"location":"api/utils/#deployer.utils.config.VertexPipelinesSettings","title":"VertexPipelinesSettings","text":"<p>               Bases: <code>BaseSettings</code></p> Source code in <code>deployer/utils/config.py</code> <pre><code>class VertexPipelinesSettings(BaseSettings):  # noqa: D101\n    model_config = SettingsConfigDict(extra=\"ignore\", case_sensitive=True)\n\n    PROJECT_ID: str\n    GCP_REGION: str\n    GAR_LOCATION: str\n    GAR_PIPELINES_REPO_ID: str\n    VERTEX_STAGING_BUCKET_NAME: str\n    VERTEX_SERVICE_ACCOUNT: str\n</code></pre>"},{"location":"api/utils/#deployer.utils.config.load_vertex_settings","title":"load_vertex_settings","text":"<pre><code>load_vertex_settings(\n    env_file: Optional[Path] = None,\n) -&gt; VertexPipelinesSettings\n</code></pre> <p>Load the settings from the environment.</p> Source code in <code>deployer/utils/config.py</code> <pre><code>def load_vertex_settings(env_file: Optional[Path] = None) -&gt; VertexPipelinesSettings:\n    \"\"\"Load the settings from the environment.\"\"\"\n    try:\n        settings = VertexPipelinesSettings(_env_file=env_file, _env_file_encoding=\"utf-8\")\n    except ValidationError as e:\n        msg = \"Validation failed for VertexPipelinesSettings. \"\n        if env_file is not None:\n            msg += f\"Please check your `.env` file: `{env_file}`\"\n        else:\n            msg += \"No `.env` file provided. Please check your environment variables\"\n        msg += f\"\\n{e}\"\n        raise ValueError(msg) from e\n    return settings\n</code></pre>"},{"location":"api/utils/#deployer.utils.config.list_config_filepaths","title":"list_config_filepaths","text":"<pre><code>list_config_filepaths(\n    configs_root_path: Path, pipeline_name: str\n) -&gt; List[Path]\n</code></pre> <p>List the config filepaths for a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>configs_root_path</code> <code>Path</code> <p>A <code>Path</code> object representing the root path of the configs.</p> required <code>pipeline_name</code> <code>str</code> <p>The name of the pipeline.</p> required <p>Returns:</p> Type Description <code>List[Path]</code> <p>List[Path]: A list of <code>Path</code> objects representing the config filepaths.</p> Source code in <code>deployer/utils/config.py</code> <pre><code>def list_config_filepaths(configs_root_path: Path, pipeline_name: str) -&gt; List[Path]:\n    \"\"\"List the config filepaths for a pipeline.\n\n    Args:\n        configs_root_path (Path): A `Path` object representing the root path of the configs.\n        pipeline_name (str): The name of the pipeline.\n\n    Returns:\n        List[Path]: A list of `Path` objects representing the config filepaths.\n    \"\"\"\n    configs_dirpath = Path(configs_root_path) / pipeline_name\n    config_filepaths = [\n        x\n        for config_type in ConfigType.__members__.keys()\n        for x in configs_dirpath.glob(f\"*.{config_type}\")\n    ]\n    return config_filepaths\n</code></pre>"},{"location":"api/utils/#deployer.utils.config.load_config","title":"load_config","text":"<pre><code>load_config(\n    config_filepath: Path,\n) -&gt; Tuple[Optional[dict], Optional[dict]]\n</code></pre> <p>Load the parameter values and input artifacts from a config file.</p> <p>Config file can be a JSON or Python file.     - If JSON, it should be a dict of parameter values.     - If Python, it should contain a <code>parameter_values</code> dict     and / or an <code>input_artifacts</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>config_filepath</code> <code>Path</code> <p>A <code>Path</code> object representing the path to the config file.</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[dict], Optional[dict]]</code> <p>Tuple[Optional[dict], Optional[dict]]:: A tuple containing the loaded parameter values and input artifacts (or <code>None</code> if not available).</p> <p>Raises:</p> Type Description <code>UnsupportedConfigFileError</code> <p>If the file has an unsupported extension.</p> Source code in <code>deployer/utils/config.py</code> <pre><code>def load_config(config_filepath: Path) -&gt; Tuple[Optional[dict], Optional[dict]]:\n    \"\"\"Load the parameter values and input artifacts from a config file.\n\n    Config file can be a JSON or Python file.\n        - If JSON, it should be a dict of parameter values.\n        - If Python, it should contain a `parameter_values` dict\n        and / or an `input_artifacts` dict.\n\n    Args:\n        config_filepath (Path): A `Path` object representing the path to the config file.\n\n    Returns:\n        Tuple[Optional[dict], Optional[dict]]:: A tuple containing the loaded parameter values\n            and input artifacts (or `None` if not available).\n\n    Raises:\n        UnsupportedConfigFileError: If the file has an unsupported extension.\n    \"\"\"\n    config_filepath = Path(config_filepath)\n\n    if config_filepath.suffix == \".json\":\n        with open(config_filepath, \"r\") as f:\n            parameter_values = json.load(f)\n        return parameter_values, None\n\n    if config_filepath.suffix == \".toml\":\n        parameter_values = _load_config_toml(config_filepath)\n        return parameter_values, None\n\n    if config_filepath.suffix == \".yaml\" or config_filepath.suffix == \".yml\":\n        parameter_values = _load_config_yaml(config_filepath)\n        return parameter_values, None\n\n    if config_filepath.suffix == \".py\":\n        parameter_values, input_artifacts = _load_config_python(config_filepath)\n        return parameter_values, input_artifacts\n\n    raise UnsupportedConfigFileError(\n        f\"{config_filepath}: Config file extension '{config_filepath.suffix}' is not supported.\"\n        f\" Supported config file types are: {', '.join(ConfigType.__members__.values())}\"\n    )\n</code></pre>"},{"location":"api/utils/#deployer.utils.config._load_config_python","title":"_load_config_python","text":"<pre><code>_load_config_python(\n    config_filepath: Path,\n) -&gt; Tuple[Optional[dict], Optional[dict]]\n</code></pre> <p>Load the parameter values and input artifacts from a Python config file.</p> <p>Parameters:</p> Name Type Description Default <code>config_filepath</code> <code>Path</code> <p>A <code>Path</code> object representing the path to the config file.</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[dict], Optional[dict]]</code> <p>Tuple[Optional[dict], Optional[dict]]: A tuple containing the loaded parameter values (or <code>None</code> if not available) and input artifacts (or <code>None</code> if not available).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the config file does not contain a <code>parameter_values</code> and/or <code>input_artifacts</code> dict.</p> <code>ValueError</code> <p>If the config file contains common keys in <code>parameter_values</code> and <code>input_artifacts</code> dict.</p> Source code in <code>deployer/utils/config.py</code> <pre><code>def _load_config_python(config_filepath: Path) -&gt; Tuple[Optional[dict], Optional[dict]]:\n    \"\"\"Load the parameter values and input artifacts from a Python config file.\n\n    Args:\n        config_filepath (Path): A `Path` object representing the path to the config file.\n\n    Returns:\n        Tuple[Optional[dict], Optional[dict]]: A tuple containing the loaded parameter values\n            (or `None` if not available) and input artifacts (or `None` if not available).\n\n    Raises:\n        ValueError: If the config file does not contain a `parameter_values` and/or\n            `input_artifacts` dict.\n        ValueError: If the config file contains common keys in `parameter_values` and\n            `input_artifacts` dict.\n    \"\"\"\n    spec = importlib.util.spec_from_file_location(\"module.name\", config_filepath)\n    module = importlib.util.module_from_spec(spec)\n    try:\n        spec.loader.exec_module(module)\n    except Exception as e:\n        raise BadConfigError(\n            f\"{config_filepath}: invalid Python config file.\\n{e.__class__.__name__}: {e}\"\n        ) from e\n\n    parameter_values = getattr(module, \"parameter_values\", None)\n    input_artifacts = getattr(module, \"input_artifacts\", None)\n\n    if parameter_values is None and input_artifacts is None:\n        raise BadConfigError(\n            f\"{config_filepath}: Python config file must contain a `parameter_values` \"\n            \"and/or `input_artifacts` dict.\"\n        )\n\n    if parameter_values is not None and input_artifacts is not None:\n        common_keys = set(parameter_values.keys()).intersection(set(input_artifacts.keys()))\n        if common_keys:\n            raise BadConfigError(\n                f\"{config_filepath}: Python config file must not contain common keys in \"\n                \"`parameter_values` and `input_artifacts` dict. Common keys: {common_keys}\"\n            )\n\n    return parameter_values, input_artifacts\n</code></pre>"},{"location":"api/utils/#deployer.utils.config._load_config_yaml","title":"_load_config_yaml","text":"<pre><code>_load_config_yaml(config_filepath: Path) -&gt; dict\n</code></pre> <p>Load the parameter values from a YAML config file.</p> <p>Parameters:</p> Name Type Description Default <code>config_filepath</code> <code>Path</code> <p>A <code>Path</code> object representing the path to the config file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The loaded parameter values.</p> Source code in <code>deployer/utils/config.py</code> <pre><code>def _load_config_yaml(config_filepath: Path) -&gt; dict:\n    \"\"\"Load the parameter values from a YAML config file.\n\n    Args:\n        config_filepath (Path): A `Path` object representing the path to the config file.\n\n    Returns:\n        dict: The loaded parameter values.\n    \"\"\"\n    with open(config_filepath, \"r\") as f:\n        try:\n            parameter_values = yaml.safe_load(f)\n        except yaml.YAMLError as e:\n            raise BadConfigError(\n                f\"{config_filepath}: invalid YAML config file.\\n{e.__class__.__name__}: {e}\"\n            ) from e\n    return parameter_values\n</code></pre>"},{"location":"api/utils/#deployer.utils.config._load_config_toml","title":"_load_config_toml","text":"<pre><code>_load_config_toml(config_filepath: Path) -&gt; dict\n</code></pre> <p>Load the parameter values from a TOML config file.</p> <p>Parameters:</p> Name Type Description Default <code>config_filepath</code> <code>Path</code> <p>A <code>Path</code> object representing the path to the config file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The loaded parameter values.</p> Source code in <code>deployer/utils/config.py</code> <pre><code>def _load_config_toml(config_filepath: Path) -&gt; dict:\n    \"\"\"Load the parameter values from a TOML config file.\n\n    Args:\n        config_filepath (Path): A `Path` object representing the path to the config file.\n\n    Returns:\n        dict: The loaded parameter values.\n    \"\"\"\n\n    def flatten_toml_document(\n        d_: Union[TOMLDocument, tomlkit.items.Table],\n        parent_key: Optional[str] = None,\n        sep: str = \".\",\n    ) -&gt; dict:\n        \"\"\"Flatten a tomlkit.TOMLDocument. Inline tables are not flattened\"\"\"\n        items = []\n        for k, v in d_.items():\n            child_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, tomlkit.items.Table):\n                # inline tables will not be flattened\n                items.extend(flatten_toml_document(v, child_key, sep=sep).items())\n            else:\n                items.append((child_key, v))\n        return dict(items)\n\n    config_file = TOMLFile(config_filepath)\n\n    try:\n        config = config_file.read()\n        parameter_values = flatten_toml_document(config, sep=\"_\")\n    except Exception as e:\n        raise BadConfigError(\n            f\"{config_filepath}: invalid TOML config file.\\n{e.__class__.__name__}: {e}\"\n        ) from e\n\n    return parameter_values\n</code></pre>"},{"location":"api/vertex_deployer/","title":"VertexPipelineDeployer","text":""},{"location":"api/vertex_deployer/#deployer.pipeline_deployer.VertexPipelineDeployer","title":"deployer.pipeline_deployer.VertexPipelineDeployer","text":"<p>Deployer for Vertex Pipelines</p> Source code in <code>deployer/pipeline_deployer.py</code> <pre><code>class VertexPipelineDeployer:\n    \"\"\"Deployer for Vertex Pipelines\"\"\"\n\n    def __init__(\n        self,\n        pipeline_name: str,\n        pipeline_func: Callable,\n        run_name: Optional[str] = None,\n        project_id: Optional[str] = None,\n        region: Optional[str] = None,\n        staging_bucket_name: Optional[str] = None,\n        service_account: Optional[str] = None,\n        gar_location: Optional[str] = None,\n        gar_repo_id: Optional[str] = None,\n        local_package_path: Optional[Path] = None,\n    ) -&gt; None:\n        \"\"\"I don't want to write a dostring here but ruff wants me to\"\"\"\n        self.project_id = project_id\n        self.region = region\n        self.staging_bucket_name = staging_bucket_name\n        self.service_account = service_account\n\n        self.pipeline_name = pipeline_name\n        self.run_name = run_name\n        self.pipeline_func = pipeline_func\n\n        self.gar_location = gar_location\n        self.gar_repo_id = gar_repo_id\n        self.local_package_path = Path(local_package_path)\n\n        self.template_name = None\n        self.version_name = None\n\n        aiplatform.init(\n            project=self.project_id,\n            staging_bucket=f\"gs://{self.staging_bucket_name}\",\n        )\n\n    @property\n    def gar_host(self) -&gt; Optional[str]:\n        \"\"\"Return the Artifact Registry host if the location and repo ID are provided\"\"\"\n        if self.gar_location is not None and self.gar_repo_id is not None:\n            return os.path.join(\n                f\"https://{self.gar_location}-kfp.pkg.dev\", self.project_id, self.gar_repo_id\n            )\n        logger.debug(\n            \"No Artifact Registry location or repo ID provided: not using Artifact Registry\"\n        )\n        return None\n\n    @property\n    def staging_bucket_uri(self) -&gt; str:  # noqa: D102\n        return f\"gs://{self.staging_bucket_name}/root\"\n\n    def _get_template_path(self, tag: Optional[str] = None) -&gt; str:\n        \"\"\"Return the path to the pipeline template\n\n        If the Artifact Registry host is provided, return the path to the pipeline template in\n        the Artifact Registry. Otherwise, return the path to the pipeline template in the\n        local package.\n        \"\"\"\n        if self.gar_host is not None:\n            if self.template_name is not None and self.version_name is not None:\n                return os.path.join(self.gar_host, self.template_name, self.version_name)\n\n            if tag:\n                return os.path.join(self.gar_host, self.pipeline_name.replace(\"_\", \"-\"), tag)\n\n            logger.warning(\n                \"tag or template_name and version_name not provided.\"\n                \" Falling back to local package.\"\n            )\n\n        return os.path.join(str(self.local_package_path), f\"{self.pipeline_name}.yaml\")\n\n    def _check_gar_host(self) -&gt; None:\n        if self.gar_host is None:\n            raise MissingGoogleArtifactRegistryHostError(\n                \"Google Artifact Registry host is missing. \"\n                \"Please provide gar_location and gar_repo_id.\"\n            )\n\n    def _check_experiment_name(self, experiment_name: Optional[str] = None) -&gt; str:\n        if experiment_name is None:\n            experiment_name = f\"{self.pipeline_name}-experiment\".replace(\"_\", \"-\")\n            logger.info(f\"Experiment name not provided, using {experiment_name}\")\n        else:\n            experiment_name = experiment_name.replace(\"_\", \"-\")\n\n        return experiment_name\n\n    def _check_run_name(self, tag: Optional[str] = None) -&gt; None:\n        \"\"\"Each run name (job_id) must be unique.\n        We thus always add a timestamp to ensure uniqueness.\n        \"\"\"\n        now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        if self.run_name is None:\n            self.run_name = f\"{self.pipeline_name}\"\n            if tag:\n                self.run_name += f\"-{tag}\"\n\n        self.run_name = self.run_name.replace(\"_\", \"-\")\n        self.run_name += f\"-{now_str}\"\n\n        if not constants.VALID_RUN_NAME_PATTERN.match(self.run_name):\n            raise ValueError(\n                f\"Run name {self.run_name} does not match the pattern\"\n                f\" {constants.VALID_RUN_NAME_PATTERN.pattern}\"\n            )\n        logger.debug(f\"run_name is: {self.run_name}\")\n\n    def _create_pipeline_job(\n        self,\n        template_path: str,\n        enable_caching: Optional[bool] = None,\n        parameter_values: Optional[dict] = None,\n        input_artifacts: Optional[dict] = None,\n    ) -&gt; aiplatform.PipelineJob:\n        \"\"\"Create a pipeline job object\n\n        Args:\n            template_path (str): The path of PipelineJob or PipelineSpec JSON or YAML file. If the\n                Artifact Registry host is provided, this is the path to the pipeline template in\n                the Artifact Registry. Otherwise, this is the path to the pipeline template in\n                the local package.\n            enable_caching (Optional[bool], optional): Whether to turn on caching for the run.\n                If this is not set, defaults to the compile time settings, which are True for all\n                tasks by default, while users may specify different caching options for individual\n                tasks.\n                If this is set, the setting applies to all tasks in the pipeline.\n                Overrides the compile time settings. Defaults to None.\n            parameter_values (Optional[dict], optional): The mapping from runtime parameter names\n                to its values that control the pipeline run. Defaults to None.\n            input_artifacts (Optional[dict], optional): The mapping from the runtime parameter\n                name for this artifact to its resource id.\n                For example: \"vertex_model\":\"456\".\n                Note: full resource name (\"projects/123/locations/us-central1/metadataStores/default/artifacts/456\")\n                    cannot be used. Defaults to None.\n\n        Returns:\n            aiplatform.PipelineJob: The pipeline job object\n        \"\"\"  # noqa: E501\n        job = aiplatform.PipelineJob(\n            display_name=self.pipeline_name,\n            job_id=self.run_name,\n            template_path=template_path,\n            pipeline_root=self.staging_bucket_uri,\n            location=self.region,\n            enable_caching=enable_caching,\n            parameter_values=parameter_values,\n            input_artifacts=input_artifacts,\n        )\n        return job\n\n    def compile(self) -&gt; VertexPipelineDeployer:\n        \"\"\"Compile pipeline and save it to the local package path using kfp compiler\"\"\"\n        self.local_package_path.mkdir(parents=True, exist_ok=True)\n        pipeline_filepath = self.local_package_path / f\"{self.pipeline_name}.yaml\"\n\n        compiler.Compiler().compile(\n            pipeline_func=self.pipeline_func,\n            package_path=str(pipeline_filepath),\n        )\n        logger.info(f\"Pipeline {self.pipeline_name} compiled to {pipeline_filepath}\")\n\n        return self\n\n    def upload_to_registry(\n        self,\n        tags: List[str] = [\"latest\"],  # noqa: B006\n    ) -&gt; VertexPipelineDeployer:\n        \"\"\"Upload pipeline to Artifact Registry\"\"\"\n        self._check_gar_host()\n        client = RegistryClient(host=self.gar_host)\n        template_name, version_name = client.upload_pipeline(\n            file_name=self.local_package_path / f\"{self.pipeline_name}.yaml\",\n            tags=tags,\n        )\n        logger.info(f\"Pipeline {self.pipeline_name} uploaded to {self.gar_host} with tags {tags}\")\n        self.template_name = template_name\n        self.version_name = version_name\n        return self\n\n    def run(\n        self,\n        enable_caching: Optional[bool] = None,\n        parameter_values: Optional[dict] = None,\n        input_artifacts: Optional[dict] = None,\n        experiment_name: Optional[str] = None,\n        tag: Optional[str] = None,\n    ) -&gt; VertexPipelineDeployer:\n        \"\"\"Run pipeline on Vertex AI Pipelines\n\n        If the experiment name is not provided, use the pipeline name with the suffix\n        \"-experiment\". Compiled pipeline file is the one uploaded on artifact registry if the\n        host is provided, and if either the tag or the template_name and version_name are\n        provided. Otherwise, use the pipeline file in the local package.\n\n        Args:\n            enable_caching (Optional[bool], optional): Whether to turn on caching for the run.\n                If this is not set, defaults to the compile time settings, which are True for all\n                tasks by default, while users may specify different caching options for individual\n                tasks.\n                If this is set, the setting applies to all tasks in the pipeline.\n                Overrides the compile time settings. Defaults to None.\n            parameter_values (Optional[dict], optional): The mapping from runtime parameter names\n                to its values that control the pipeline run. Defaults to None.\n            input_artifacts (Optional[dict], optional): The mapping from the runtime parameter\n                name for this artifact to its resource id.\n                For example: \"vertex_model\":\"456\".\n                Note: full resource name (\"projects/123/locations/us-central1/metadataStores/default/artifacts/456\")\n                    cannot be used. Defaults to None.\n            experiment_name (str, optional): Experiment name. Defaults to None.\n            tag (str, optional): Tag of the pipeline template. Defaults to None.\n        \"\"\"  # noqa: E501\n        experiment_name = self._check_experiment_name(experiment_name)\n        self._check_run_name(tag=tag)\n        template_path = self._get_template_path(tag)\n\n        logger.debug(\n            f\"Running pipeline '{self.pipeline_name}' with settings:\"\n            f\"\\n {'template_path':&lt;20} {template_path:&lt;30}\"\n            f\"\\n {'enable_caching':&lt;20} {enable_caching!s:&lt;30}\"\n            f\"\\n {'experiment_name':&lt;20} {experiment_name:&lt;30}\"\n        )\n\n        job = self._create_pipeline_job(\n            template_path=template_path,\n            enable_caching=enable_caching,\n            parameter_values=parameter_values,\n            input_artifacts=input_artifacts,\n        )\n\n        try:\n            job.submit(\n                experiment=experiment_name,\n                service_account=self.service_account,\n            )\n        except RuntimeError as e:  # HACK: This is a temporary fix\n            if \"could not be associated with Experiment\" in str(e):\n                logger.warning(\n                    f\"Encountered an error while linking your job {job.job_id}\"\n                    f\" with experiment {experiment_name}.\"\n                    \" This is likely due to a bug in the AI Platform Pipelines client.\"\n                    \" Your job should be running anyway. Try to link it manually.\"\n                )\n            else:\n                raise e\n\n        return self\n\n    def compile_upload_run(\n        self,\n        enable_caching: Optional[bool] = None,\n        parameter_values: Optional[dict] = None,\n        experiment_name: Optional[str] = None,\n        tags: Optional[List[str]] = None,\n    ) -&gt; VertexPipelineDeployer:\n        \"\"\"Compile, upload and run pipeline on Vertex AI Pipelines\"\"\"\n        self.compile()\n\n        if self.gar_host is not None:\n            self.upload_to_registry(tags)\n\n        self.run(\n            enable_caching=enable_caching,\n            parameter_values=parameter_values,\n            experiment_name=experiment_name,\n            tag=tags[0] if tags else None,\n        )\n        return self\n\n    def schedule(\n        self,\n        cron: str,\n        enable_caching: Optional[bool] = None,\n        parameter_values: Optional[dict] = None,\n        tag: Optional[str] = None,\n        delete_last_schedule: bool = False,\n        scheduler_timezone: str = \"Europe/Paris\",\n    ) -&gt; VertexPipelineDeployer:\n        \"\"\"Create pipeline schedule on Vertex AI Pipelines\n\n        Compiled pipeline file is the one uploaded on artifact registry if the host is provided,\n        and if either the tag or the template_name and version_name are provided.\n\n        Args:\n            cron (str): Cron expression without TZ.\n            enable_caching (bool, optional): Whether to enable caching. Defaults to False.\n            parameter_values (dict, optional): Pipeline parameter values. Defaults to None.\n            tag (str, optional): Tag of the pipeline template. Defaults to None.\n            delete_last_schedule (bool, optional): Whether to delete previous schedule.\n                Defaults to False.\n            scheduler_timezone (str, optional): Scheduler timezone. Must be a valid string from\n                IANA time zone database. Defaults to 'Europe/Paris'.\n        \"\"\"\n        self._check_gar_host()\n\n        schedule_display_name = f\"schedule-{self.pipeline_name}\"\n        schedules_list = PipelineJobSchedule.list(\n            filter=f'display_name=\"{schedule_display_name}\"',\n            order_by=\"create_time desc\",\n            location=self.region,\n        )\n\n        logger.info(\n            f\"There are {len(schedules_list)} schedules defined for pipeline {self.pipeline_name}\"\n        )\n        if len(schedules_list) &gt; 0 and delete_last_schedule:\n            logger.info(\n                f\"Deleting schedule {schedules_list[0].display_name}\"\n                f\" for pipeline {self.pipeline_name} at {schedules_list[0].cron}\"\n            )\n            schedules_list[0].delete()\n\n        if tag:\n            client = RegistryClient(host=self.gar_host)\n            package_name = self.pipeline_name.replace(\"_\", \"-\")\n            try:\n                tag_metadata = client.get_tag(package_name=package_name, tag=tag)\n            except HTTPError as e:\n                tags_list = client.list_tags(package_name)\n                tags_list_parsed = [x[\"name\"].split(\"/\")[-1] for x in tags_list]\n                raise TagNotFoundError(\n                    f\"Tag {tag} not found for package {self.gar_host}/{package_name}.\\\n                        Available tags: {tags_list_parsed}\"\n                ) from e\n\n            pipeline_version_sha = tag_metadata[\"version\"].split(\"/\")[-1]\n\n            template_path = self._get_template_path(pipeline_version_sha)\n        else:\n            template_path = self._get_template_path()\n\n        logger.info(\n            f\"Creating schedule for pipeline {self.pipeline_name} at {cron}\"\n            f\" with template {template_path}\"\n        )\n\n        job = self._create_pipeline_job(\n            template_path=template_path,\n            enable_caching=enable_caching,\n            parameter_values=parameter_values,\n        )\n\n        # HACK: Must set location or it will default to \"us-central1\" (or project default)\n        pipeline_job_schedule = PipelineJobSchedule(\n            pipeline_job=job,\n            display_name=schedule_display_name,\n            location=self.region,\n        )\n\n        pipeline_job_schedule.create(\n            cron=f\"TZ={scheduler_timezone} {cron}\",\n            service_account=self.service_account,\n        )\n\n        return self\n</code></pre>"},{"location":"api/vertex_deployer/#deployer.pipeline_deployer.VertexPipelineDeployer.gar_host","title":"gar_host  <code>property</code>","text":"<pre><code>gar_host: Optional[str]\n</code></pre> <p>Return the Artifact Registry host if the location and repo ID are provided</p>"},{"location":"api/vertex_deployer/#deployer.pipeline_deployer.VertexPipelineDeployer.compile","title":"compile","text":"<pre><code>compile() -&gt; VertexPipelineDeployer\n</code></pre> <p>Compile pipeline and save it to the local package path using kfp compiler</p> Source code in <code>deployer/pipeline_deployer.py</code> <pre><code>def compile(self) -&gt; VertexPipelineDeployer:\n    \"\"\"Compile pipeline and save it to the local package path using kfp compiler\"\"\"\n    self.local_package_path.mkdir(parents=True, exist_ok=True)\n    pipeline_filepath = self.local_package_path / f\"{self.pipeline_name}.yaml\"\n\n    compiler.Compiler().compile(\n        pipeline_func=self.pipeline_func,\n        package_path=str(pipeline_filepath),\n    )\n    logger.info(f\"Pipeline {self.pipeline_name} compiled to {pipeline_filepath}\")\n\n    return self\n</code></pre>"},{"location":"api/vertex_deployer/#deployer.pipeline_deployer.VertexPipelineDeployer.upload_to_registry","title":"upload_to_registry","text":"<pre><code>upload_to_registry(\n    tags: List[str] = [\"latest\"],\n) -&gt; VertexPipelineDeployer\n</code></pre> <p>Upload pipeline to Artifact Registry</p> Source code in <code>deployer/pipeline_deployer.py</code> <pre><code>def upload_to_registry(\n    self,\n    tags: List[str] = [\"latest\"],  # noqa: B006\n) -&gt; VertexPipelineDeployer:\n    \"\"\"Upload pipeline to Artifact Registry\"\"\"\n    self._check_gar_host()\n    client = RegistryClient(host=self.gar_host)\n    template_name, version_name = client.upload_pipeline(\n        file_name=self.local_package_path / f\"{self.pipeline_name}.yaml\",\n        tags=tags,\n    )\n    logger.info(f\"Pipeline {self.pipeline_name} uploaded to {self.gar_host} with tags {tags}\")\n    self.template_name = template_name\n    self.version_name = version_name\n    return self\n</code></pre>"},{"location":"api/vertex_deployer/#deployer.pipeline_deployer.VertexPipelineDeployer.run","title":"run","text":"<pre><code>run(\n    enable_caching: Optional[bool] = None,\n    parameter_values: Optional[dict] = None,\n    input_artifacts: Optional[dict] = None,\n    experiment_name: Optional[str] = None,\n    tag: Optional[str] = None,\n) -&gt; VertexPipelineDeployer\n</code></pre> <p>Run pipeline on Vertex AI Pipelines</p> <p>If the experiment name is not provided, use the pipeline name with the suffix \"-experiment\". Compiled pipeline file is the one uploaded on artifact registry if the host is provided, and if either the tag or the template_name and version_name are provided. Otherwise, use the pipeline file in the local package.</p> <p>Parameters:</p> Name Type Description Default <code>enable_caching</code> <code>Optional[bool]</code> <p>Whether to turn on caching for the run. If this is not set, defaults to the compile time settings, which are True for all tasks by default, while users may specify different caching options for individual tasks. If this is set, the setting applies to all tasks in the pipeline. Overrides the compile time settings. Defaults to None.</p> <code>None</code> <code>parameter_values</code> <code>Optional[dict]</code> <p>The mapping from runtime parameter names to its values that control the pipeline run. Defaults to None.</p> <code>None</code> <code>input_artifacts</code> <code>Optional[dict]</code> <p>The mapping from the runtime parameter name for this artifact to its resource id. For example: \"vertex_model\":\"456\". Note: full resource name (\"projects/123/locations/us-central1/metadataStores/default/artifacts/456\")     cannot be used. Defaults to None.</p> <code>None</code> <code>experiment_name</code> <code>str</code> <p>Experiment name. Defaults to None.</p> <code>None</code> <code>tag</code> <code>str</code> <p>Tag of the pipeline template. Defaults to None.</p> <code>None</code> Source code in <code>deployer/pipeline_deployer.py</code> <pre><code>def run(\n    self,\n    enable_caching: Optional[bool] = None,\n    parameter_values: Optional[dict] = None,\n    input_artifacts: Optional[dict] = None,\n    experiment_name: Optional[str] = None,\n    tag: Optional[str] = None,\n) -&gt; VertexPipelineDeployer:\n    \"\"\"Run pipeline on Vertex AI Pipelines\n\n    If the experiment name is not provided, use the pipeline name with the suffix\n    \"-experiment\". Compiled pipeline file is the one uploaded on artifact registry if the\n    host is provided, and if either the tag or the template_name and version_name are\n    provided. Otherwise, use the pipeline file in the local package.\n\n    Args:\n        enable_caching (Optional[bool], optional): Whether to turn on caching for the run.\n            If this is not set, defaults to the compile time settings, which are True for all\n            tasks by default, while users may specify different caching options for individual\n            tasks.\n            If this is set, the setting applies to all tasks in the pipeline.\n            Overrides the compile time settings. Defaults to None.\n        parameter_values (Optional[dict], optional): The mapping from runtime parameter names\n            to its values that control the pipeline run. Defaults to None.\n        input_artifacts (Optional[dict], optional): The mapping from the runtime parameter\n            name for this artifact to its resource id.\n            For example: \"vertex_model\":\"456\".\n            Note: full resource name (\"projects/123/locations/us-central1/metadataStores/default/artifacts/456\")\n                cannot be used. Defaults to None.\n        experiment_name (str, optional): Experiment name. Defaults to None.\n        tag (str, optional): Tag of the pipeline template. Defaults to None.\n    \"\"\"  # noqa: E501\n    experiment_name = self._check_experiment_name(experiment_name)\n    self._check_run_name(tag=tag)\n    template_path = self._get_template_path(tag)\n\n    logger.debug(\n        f\"Running pipeline '{self.pipeline_name}' with settings:\"\n        f\"\\n {'template_path':&lt;20} {template_path:&lt;30}\"\n        f\"\\n {'enable_caching':&lt;20} {enable_caching!s:&lt;30}\"\n        f\"\\n {'experiment_name':&lt;20} {experiment_name:&lt;30}\"\n    )\n\n    job = self._create_pipeline_job(\n        template_path=template_path,\n        enable_caching=enable_caching,\n        parameter_values=parameter_values,\n        input_artifacts=input_artifacts,\n    )\n\n    try:\n        job.submit(\n            experiment=experiment_name,\n            service_account=self.service_account,\n        )\n    except RuntimeError as e:  # HACK: This is a temporary fix\n        if \"could not be associated with Experiment\" in str(e):\n            logger.warning(\n                f\"Encountered an error while linking your job {job.job_id}\"\n                f\" with experiment {experiment_name}.\"\n                \" This is likely due to a bug in the AI Platform Pipelines client.\"\n                \" Your job should be running anyway. Try to link it manually.\"\n            )\n        else:\n            raise e\n\n    return self\n</code></pre>"},{"location":"api/vertex_deployer/#deployer.pipeline_deployer.VertexPipelineDeployer.compile_upload_run","title":"compile_upload_run","text":"<pre><code>compile_upload_run(\n    enable_caching: Optional[bool] = None,\n    parameter_values: Optional[dict] = None,\n    experiment_name: Optional[str] = None,\n    tags: Optional[List[str]] = None,\n) -&gt; VertexPipelineDeployer\n</code></pre> <p>Compile, upload and run pipeline on Vertex AI Pipelines</p> Source code in <code>deployer/pipeline_deployer.py</code> <pre><code>def compile_upload_run(\n    self,\n    enable_caching: Optional[bool] = None,\n    parameter_values: Optional[dict] = None,\n    experiment_name: Optional[str] = None,\n    tags: Optional[List[str]] = None,\n) -&gt; VertexPipelineDeployer:\n    \"\"\"Compile, upload and run pipeline on Vertex AI Pipelines\"\"\"\n    self.compile()\n\n    if self.gar_host is not None:\n        self.upload_to_registry(tags)\n\n    self.run(\n        enable_caching=enable_caching,\n        parameter_values=parameter_values,\n        experiment_name=experiment_name,\n        tag=tags[0] if tags else None,\n    )\n    return self\n</code></pre>"},{"location":"api/vertex_deployer/#deployer.pipeline_deployer.VertexPipelineDeployer.schedule","title":"schedule","text":"<pre><code>schedule(\n    cron: str,\n    enable_caching: Optional[bool] = None,\n    parameter_values: Optional[dict] = None,\n    tag: Optional[str] = None,\n    delete_last_schedule: bool = False,\n    scheduler_timezone: str = \"Europe/Paris\",\n) -&gt; VertexPipelineDeployer\n</code></pre> <p>Create pipeline schedule on Vertex AI Pipelines</p> <p>Compiled pipeline file is the one uploaded on artifact registry if the host is provided, and if either the tag or the template_name and version_name are provided.</p> <p>Parameters:</p> Name Type Description Default <code>cron</code> <code>str</code> <p>Cron expression without TZ.</p> required <code>enable_caching</code> <code>bool</code> <p>Whether to enable caching. Defaults to False.</p> <code>None</code> <code>parameter_values</code> <code>dict</code> <p>Pipeline parameter values. Defaults to None.</p> <code>None</code> <code>tag</code> <code>str</code> <p>Tag of the pipeline template. Defaults to None.</p> <code>None</code> <code>delete_last_schedule</code> <code>bool</code> <p>Whether to delete previous schedule. Defaults to False.</p> <code>False</code> <code>scheduler_timezone</code> <code>str</code> <p>Scheduler timezone. Must be a valid string from IANA time zone database. Defaults to 'Europe/Paris'.</p> <code>'Europe/Paris'</code> Source code in <code>deployer/pipeline_deployer.py</code> <pre><code>def schedule(\n    self,\n    cron: str,\n    enable_caching: Optional[bool] = None,\n    parameter_values: Optional[dict] = None,\n    tag: Optional[str] = None,\n    delete_last_schedule: bool = False,\n    scheduler_timezone: str = \"Europe/Paris\",\n) -&gt; VertexPipelineDeployer:\n    \"\"\"Create pipeline schedule on Vertex AI Pipelines\n\n    Compiled pipeline file is the one uploaded on artifact registry if the host is provided,\n    and if either the tag or the template_name and version_name are provided.\n\n    Args:\n        cron (str): Cron expression without TZ.\n        enable_caching (bool, optional): Whether to enable caching. Defaults to False.\n        parameter_values (dict, optional): Pipeline parameter values. Defaults to None.\n        tag (str, optional): Tag of the pipeline template. Defaults to None.\n        delete_last_schedule (bool, optional): Whether to delete previous schedule.\n            Defaults to False.\n        scheduler_timezone (str, optional): Scheduler timezone. Must be a valid string from\n            IANA time zone database. Defaults to 'Europe/Paris'.\n    \"\"\"\n    self._check_gar_host()\n\n    schedule_display_name = f\"schedule-{self.pipeline_name}\"\n    schedules_list = PipelineJobSchedule.list(\n        filter=f'display_name=\"{schedule_display_name}\"',\n        order_by=\"create_time desc\",\n        location=self.region,\n    )\n\n    logger.info(\n        f\"There are {len(schedules_list)} schedules defined for pipeline {self.pipeline_name}\"\n    )\n    if len(schedules_list) &gt; 0 and delete_last_schedule:\n        logger.info(\n            f\"Deleting schedule {schedules_list[0].display_name}\"\n            f\" for pipeline {self.pipeline_name} at {schedules_list[0].cron}\"\n        )\n        schedules_list[0].delete()\n\n    if tag:\n        client = RegistryClient(host=self.gar_host)\n        package_name = self.pipeline_name.replace(\"_\", \"-\")\n        try:\n            tag_metadata = client.get_tag(package_name=package_name, tag=tag)\n        except HTTPError as e:\n            tags_list = client.list_tags(package_name)\n            tags_list_parsed = [x[\"name\"].split(\"/\")[-1] for x in tags_list]\n            raise TagNotFoundError(\n                f\"Tag {tag} not found for package {self.gar_host}/{package_name}.\\\n                    Available tags: {tags_list_parsed}\"\n            ) from e\n\n        pipeline_version_sha = tag_metadata[\"version\"].split(\"/\")[-1]\n\n        template_path = self._get_template_path(pipeline_version_sha)\n    else:\n        template_path = self._get_template_path()\n\n    logger.info(\n        f\"Creating schedule for pipeline {self.pipeline_name} at {cron}\"\n        f\" with template {template_path}\"\n    )\n\n    job = self._create_pipeline_job(\n        template_path=template_path,\n        enable_caching=enable_caching,\n        parameter_values=parameter_values,\n    )\n\n    # HACK: Must set location or it will default to \"us-central1\" (or project default)\n    pipeline_job_schedule = PipelineJobSchedule(\n        pipeline_job=job,\n        display_name=schedule_display_name,\n        location=self.region,\n    )\n\n    pipeline_job_schedule.create(\n        cron=f\"TZ={scheduler_timezone} {cron}\",\n        service_account=self.service_account,\n    )\n\n    return self\n</code></pre>"}]}